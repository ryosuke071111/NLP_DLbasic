{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第4回 演習（2）Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TransformerはRNNやCNNを使用せず、Attentionのみを用いるSeq2Seqモデルです。\n",
    "\n",
    "並列計算が可能なためRNNに比べて計算が高速な上、Self-Attentionと呼ばれる機構を用いることにより、局所的な位置しか参照できないCNNと異なり、系列内の任意の位置の情報を参照することを可能にしています。\n",
    "\n",
    "その他にもいくつかの工夫が加えられており、翻訳に限らない自然言語処理のあらゆるタスクで圧倒的な性能を示すことが知られています。\n",
    "\n",
    "原論文：[Attention is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "参考実装：https://github.com/jadore801120/attention-is-all-you-need-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk import bleu_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "try:\n",
    "    from utils import Vocab\n",
    "except ImportError:  # iLect環境\n",
    "    import os\n",
    "    os.chdir('/root/userspace/chap4/materials')\n",
    "    from utils import Vocab\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.manual_seed(1)\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "UNK = 1\n",
    "BOS = 2\n",
    "EOS = 3\n",
    "\n",
    "PAD_TOKEN = '<PAD>'\n",
    "UNK_TOKEN = '<UNK>'\n",
    "BOS_TOKEN = '<S>'\n",
    "EOS_TOKEN = '</S>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. データセット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    テキストファイルからデータを読み込む\n",
    "    :param file_path: str, テキストファイルのパス\n",
    "    :return data: list, 文章（単語のリスト）のリスト\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for line in open(file_path, encoding='utf-8'):\n",
    "        words = line.strip().split()  # スペースで単語を分割\n",
    "        data.append(words)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = load_data('../data/train.en')\n",
    "train_Y = load_data('../data/train.ja')\n",
    "# 演習用にデータサイズを縮小\n",
    "train_X = train_X[:len(train_X)//2]\n",
    "train_Y = train_Y[:len(train_Y)//2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 訓練データと検証データに分割\n",
    "train_X, valid_X, train_Y, valid_Y = train_test_split(train_X, train_Y, test_size=0.2, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN_COUNT = 2  # 語彙に含める単語の最低出現回数\n",
    "\n",
    "word2id = {\n",
    "    PAD_TOKEN: PAD,\n",
    "    BOS_TOKEN: BOS,\n",
    "    EOS_TOKEN: EOS,\n",
    "    UNK_TOKEN: UNK,\n",
    "    }\n",
    "\n",
    "vocab_X = Vocab(word2id=word2id)\n",
    "vocab_Y = Vocab(word2id=word2id)\n",
    "vocab_X.build_vocab(train_X, min_count=MIN_COUNT)\n",
    "vocab_Y.build_vocab(train_Y, min_count=MIN_COUNT)\n",
    "\n",
    "vocab_size_X = len(vocab_X.id2word)\n",
    "vocab_size_Y = len(vocab_Y.id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_to_ids(vocab, sentence):\n",
    "    \"\"\"\n",
    "    単語のリストをインデックスのリストに変換する\n",
    "    :param vocab: Vocabのインスタンス\n",
    "    :param sentence: list of str\n",
    "    :return indices: list of int\n",
    "    \"\"\"\n",
    "    ids = [vocab.word2id.get(word, UNK) for word in sentence]\n",
    "    ids = [BOS] + ids + [EOS]  # </S>トークンを末尾に加える\n",
    "#     ids += [EOS]  # EOSを末尾に加える\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = [sentence_to_ids(vocab_X, sentence) for sentence in train_X]\n",
    "train_Y = [sentence_to_ids(vocab_Y, sentence) for sentence in train_Y]\n",
    "valid_X = [sentence_to_ids(vocab_X, sentence) for sentence in valid_X]\n",
    "valid_Y = [sentence_to_ids(vocab_Y, sentence) for sentence in valid_Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataLoader(object):\n",
    "    def __init__(self, src_insts, tgt_insts, batch_size, shuffle=True):\n",
    "        \"\"\"\n",
    "        :param src_insts: list, 入力言語の文章（単語IDのリスト）のリスト\n",
    "        :param tgt_insts: list, 出力言語の文章（単語IDのリスト）のリスト\n",
    "        :param batch_size: int, バッチサイズ\n",
    "        :param shuffle: bool, サンプルの順番をシャッフルするか否か\n",
    "        \"\"\"\n",
    "        self.data = list(zip(src_insts, tgt_insts))\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.start_index = 0\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        if self.shuffle:\n",
    "            self.data = shuffle(self.data, random_state=random_state)\n",
    "        self.start_index = 0\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "\n",
    "        def preprocess_seqs(seqs):\n",
    "            # パディング\n",
    "            max_length = max([len(s) for s in seqs])\n",
    "            data = [s + [PAD] * (max_length - len(s)) for s in seqs]\n",
    "            # 単語の位置を表現するベクトルを作成\n",
    "            positions = [[pos+1 if w != PAD else 0 for pos, w in enumerate(seq)] for seq in data]\n",
    "            # テンソルに変換\n",
    "            data_tensor = torch.tensor(data, dtype=torch.long, device=device)\n",
    "            position_tensor = torch.tensor(positions, dtype=torch.long, device=device)\n",
    "            return data_tensor, position_tensor            \n",
    "\n",
    "        # ポインタが最後まで到達したら初期化する\n",
    "        if self.start_index >= len(self.data):\n",
    "            self.reset()\n",
    "            raise StopIteration()\n",
    "\n",
    "        # バッチを取得して前処理\n",
    "        src_seqs, tgt_seqs = zip(*self.data[self.start_index:self.start_index+self.batch_size])\n",
    "        src_data, src_pos = preprocess_seqs(src_seqs)\n",
    "        tgt_data, tgt_pos = preprocess_seqs(tgt_seqs)\n",
    "\n",
    "        # ポインタを更新する\n",
    "        self.start_index += self.batch_size\n",
    "\n",
    "        return (src_data, src_pos), (tgt_data, tgt_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 各モジュールの定義\n",
    "Transformerのモデルも大きくはEncoder(下図左側）とDecoder（下図右側）からなっており、それぞれ灰色で表されているブロックをN個積み重ねた構造となっています。\n",
    "\n",
    "EncoderとDecoderは\n",
    "- Positional Encoding: 入出力の単語のEmbedding時に単語の位置情報を埋め込む\n",
    "- Scaled Dot-Product Attention: 内積でAttentionを計算し、スケーリングを行う\n",
    "- Multi-head Attention: Scaled Dot-Product Attentionを複数のヘッドで並列化する（下図橙色）\n",
    "- Position-Wise Feed Forward Network: 単語列の位置ごとに独立して処理を行う（下図水色）\n",
    "\n",
    "など、いくつかのモジュールから構成されているため、それぞれのモジュールを定義していきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/Transformer.png\" style=\"height: 600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ① Positional Encoding\n",
    "Transformerは系列の処理にRNNやCNNを使用しないので、そのままでは単語列の語順を考慮することができません。\n",
    "\n",
    "そのため、入力系列の埋込行列に単語の位置情報を埋め込むPositional Encodingを加算します。\n",
    "\n",
    "Positional Encodingの行列${\\bf PE}$の各成分は次式で表されます。\n",
    "\n",
    "$PE_{(pos, 2i)} = \\sin(pos/10000^{2i/d_{model}})$\n",
    "\n",
    "$PE_{(pos, 2i+1)} = \\cos(pos/10000^{2i/d_{model}})$\n",
    "\n",
    "ここで$pos$は単語の位置を、$i$は成分の次元を表しています。\n",
    "\n",
    "Positional Encodingの各次元は、波長が$2\\pi$から$10000*2\\pi$に幾何学的に伸びる正弦波に対応します。\n",
    "\n",
    "未知の系列長にも対応可能であり、また学習は不要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def position_encoding_init(n_position, d_pos_vec):\n",
    "    \"\"\"\n",
    "    Positional Encodingのための行列の初期化を行う\n",
    "    :param n_position: int, 系列長\n",
    "    :param d_pos_vec: int, 隠れ層の次元数\n",
    "    :return: torch.tensor, size=(n_position, d_pos_vec)\n",
    "    \"\"\"\n",
    "    # PADがある単語の位置はpos=0にしておき、position_encも0にする\n",
    "    position_enc = np.array([\n",
    "        [pos / np.power(10000, 2 * (j // 2) / d_pos_vec) for j in range(d_pos_vec)]\n",
    "        if pos != 0 else np.zeros(d_pos_vec) for pos in range(n_position)])\n",
    "\n",
    "    position_enc[1:, 0::2] = np.sin(position_enc[1:, 0::2]) # dim 2i\n",
    "    position_enc[1:, 1::2] = np.cos(position_enc[1:, 1::2]) # dim 2i+1\n",
    "    return torch.tensor(position_enc, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positional Encodingを可視化すると以下のようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0cAAAHeCAYAAABQTHAhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdebaa97978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pe = position_encoding_init(50, 256).numpy()\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.heatmap(pe, cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "縦軸が単語の位置を、横軸が成分の次元を表しており、濃淡が加算される値です。\n",
    "\n",
    "ここでは最大系列長を50、隠れ層の次元数を256としました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ② Multihead Attention\n",
    "\n",
    "\n",
    "#### Source-Target-AttentionとSelf-Attention\n",
    "\n",
    "Attentionは一般に、queryベクトルとkeyベクトルの類似度を求めて、その正規化した重みをvalueベクトルに適用して値を取り出す処理を行います。\n",
    "\n",
    "一般的な翻訳モデルで用いられるAttentionはSource-Target-Attentionと呼ばれ、この場合queryはDecoderの隠れ状態(Target)、keyはEncoderの隠れ状態（Source）、valueもEncoderの隠れ状態（Source）で表現されるのが一般的です。\n",
    "モデル全体の図では、右側のDecoderのブロックの中央にあるAttentionがこれに該当します。\n",
    "\n",
    "\n",
    "Transformerでは、このSource-Target-Attentionに加えて、query、key、valueを同じ系列内で定義するSelf-Attentionの機構を用います。これにより、ある単語位置の出力を求める際にあらゆる位置を参照できるため、局所的な位置しか参照できない畳み込み層よりも良い性能を発揮できると言われています。\n",
    "モデル全体の図では、左側のEncoderブロックと右側のDecoderのブロックの下部にあるAttentionがこれに該当します。\n",
    "\n",
    "<img src=\"../images/Attention.png\" style=\"width:80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformerでは、Scaled Dot-Product Attentionと呼ばれるAttentionを、複数のヘッドで並列に行うMulti-Head Attentionによって、Source-Target-AttentionとSelf-Attentionを表現します。\n",
    "\n",
    "以下では、Scaled Dot-Product AttentionとMulti-Head Attentionについて順に説明していきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaled Dot-Product Attention\n",
    "Attentionには、注意の重みを隠れ層 1 つのフィードフォワードネットワークで求めるAdditive Attentionと、注意の重みを内積で求めるDot-Product Attentionが存在します。  一般に、Dot-Product Attentionのほうがパラメータが少なく高速であり、Transformerでもこちらを使います。\n",
    "\n",
    "\n",
    "Tranformerではさらなる工夫として、query($Q$)とkey($K$)の内積をスケーリング因子 $\\sqrt{d_k}$ で除算します。\n",
    "\n",
    "$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$ \n",
    "\n",
    "\n",
    "これは、$d_k$（keyベクトルの次元数）が大きい場合に内積が大きくなりすぎて逆伝播のsoftmaxの勾配が極端に小さくなることを防ぐ役割を果たします。\n",
    "\n",
    "<img src=\"../images/Multihead.png\" style=\"height: 400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_model, attn_dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param d_model: int, 隠れ層の次元数\n",
    "        :param attn_dropout: float, ドロップアウト率\n",
    "        \"\"\"\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.temper = np.power(d_model, 0.5)  # スケーリング因子\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, q, k, v, attn_mask):\n",
    "        \"\"\"\n",
    "        :param q: torch.tensor, queryベクトル, \n",
    "            size=(n_head*batch_size, len_q, d_k)\n",
    "        :param k: torch.tensor, key, \n",
    "            size=(n_head*batch_size, len_k, d_k)\n",
    "        :param v: torch.tensor, valueベクトル, \n",
    "            size=(n_head*batch_size, len_v, d_v)\n",
    "        :param attn_mask: torch.tensor, Attentionに適用するマスク, \n",
    "            size=(n_head*batch_size, len_q, len_k)\n",
    "        :return output: 出力ベクトル, \n",
    "            size=(n_head*batch_size, len_q, d_v)\n",
    "        :return attn: Attention\n",
    "            size=(n_head*batch_size, len_q, len_k)\n",
    "        \"\"\"\n",
    "        # QとKの内積(上図右側`MatMul`)でAttentionの重みを求め、スケーリングする(上図右側`Scale`)\n",
    "        attn = # WRITE ME! Hint: torch.bmmを使う (n_head*batch_size, len_q, len_k)\n",
    "        # Attentionをかけたくない部分がある場合は、その部分を負の無限大に飛ばしてSoftmaxの値が0になるようにする(上図右側`Mask(opt.)`)\n",
    "        attn.data.masked_fill_(attn_mask, -float('inf'))\n",
    "        \n",
    "        attn = self.softmax(attn)  # (上図右側`SoftMax`)\n",
    "        attn = self.dropout(attn)\n",
    "        output = # WRITE ME! attnとVの内積を計算 (上図右側`MatMul`)\n",
    "\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Head Attention\n",
    "TransformerではAttentionを複数のヘッドで並列に行うMulti-Head Attentionを採用しています。\n",
    "\n",
    "複数のヘッドでAttentionを行うことにより、各ヘッドが異なる位置の異なる部分空間を処理でき、精度が向上するとされています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param n_head: int, ヘッド数\n",
    "        :param d_model: int, 隠れ層の次元数\n",
    "        :param d_k: int, keyベクトルの次元数\n",
    "        :param d_v: int, valueベクトルの次元数\n",
    "        :param dropout: float, ドロップアウト率\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "        # 各ヘッドごとに異なる重みで線形変換を行うための重み\n",
    "        # nn.Parameterを使うことで、Moduleのパラメータとして登録できる\n",
    "        self.w_qs = nn.Parameter(torch.empty([n_head, d_model, d_k], dtype=torch.float))\n",
    "        self.w_ks = nn.Parameter(torch.empty([n_head, d_model, d_k], dtype=torch.float))\n",
    "        self.w_vs = nn.Parameter(torch.empty([n_head, d_model, d_v], dtype=torch.float))\n",
    "        # nn.init.xavier_normal_で重みの値を初期化\n",
    "        nn.init.xavier_normal_(self.w_qs)\n",
    "        nn.init.xavier_normal_(self.w_ks)\n",
    "        nn.init.xavier_normal_(self.w_vs)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(d_model)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.proj = nn.Linear(n_head*d_v, d_model)  # 複数ヘッド分のAttentionの結果を元のサイズに写像するための線形層\n",
    "        # nn.init.xavier_normal_で重みの値を初期化\n",
    "        nn.init.xavier_normal_(self.proj.weight)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, q, k, v, attn_mask=None):\n",
    "        \"\"\"\n",
    "        :param q: torch.tensor, queryベクトル, \n",
    "            size=(batch_size, len_q, d_model)\n",
    "        :param k: torch.tensor, key, \n",
    "            size=(batch_size, len_k, d_model)\n",
    "        :param v: torch.tensor, valueベクトル, \n",
    "            size=(batch_size, len_v, d_model)\n",
    "        :param attn_mask: torch.tensor, Attentionに適用するマスク, \n",
    "            size=(batch_size, len_q, len_k)\n",
    "        :return outputs: 出力ベクトル, \n",
    "            size=(batch_size, len_q, d_model)\n",
    "        :return attns: Attention\n",
    "            size=(n_head*batch_size, len_q, len_k)\n",
    "            \n",
    "        \"\"\"\n",
    "        d_k, d_v = self.d_k, self.d_v\n",
    "        n_head = self.n_head\n",
    "\n",
    "        # residual connectionのための入力\n",
    "        residual = q\n",
    "\n",
    "        batch_size, len_q, d_model = q.size()\n",
    "        batch_size, len_k, d_model = k.size()\n",
    "        batch_size, len_v, d_model = v.size()\n",
    "\n",
    "        # 複数ヘッド化\n",
    "        # torch.repeat または .repeatで指定したdimに沿って同じテンソルを作成\n",
    "        q_s = q.repeat(n_head, 1, 1) # (n_head*batch_size, len_q, d_model)\n",
    "        k_s = k.repeat(n_head, 1, 1) # (n_head*batch_size, len_k, d_model)\n",
    "        v_s = v.repeat(n_head, 1, 1) # (n_head*batch_size, len_v, d_model)\n",
    "        # ヘッドごとに並列計算させるために、n_headをdim=0に、batch_sizeをdim=1に寄せる\n",
    "        q_s = q_s.view(n_head, -1, d_model) # (n_head, batch_size*len_q, d_model)\n",
    "        k_s = k_s.view(n_head, -1, d_model) # (n_head, batch_size*len_k, d_model)\n",
    "        v_s = v_s.view(n_head, -1, d_model) # (n_head, batch_size*len_v, d_model)\n",
    "\n",
    "        # 各ヘッドで線形変換を並列計算(上図左側`Linear`)\n",
    "        q_s = torch.bmm(q_s, self.w_qs)  # (n_head, batch_size*len_q, d_k)\n",
    "        k_s = torch.bmm(k_s, self.w_ks)  # (n_head, batch_size*len_k, d_k)\n",
    "        v_s = torch.bmm(v_s, self.w_vs)  # (n_head, batch_size*len_v, d_v)\n",
    "        # Attentionは各バッチ各ヘッドごとに計算させるためにbatch_sizeをdim=0に寄せる\n",
    "        q_s = q_s.view(-1, len_q, d_k)   # (n_head*batch_size, len_q, d_k)\n",
    "        k_s = k_s.view(-1, len_k, d_k)   # (n_head*batch_size, len_k, d_k)\n",
    "        v_s = v_s.view(-1, len_v, d_v)   # (n_head*batch_size, len_v, d_v)\n",
    "\n",
    "        # Attentionを計算(上図左側`Scaled Dot-Product Attention * h`)\n",
    "        outputs, attns = self.attention(q_s, k_s, v_s, attn_mask=attn_mask.repeat(n_head, 1, 1))\n",
    "        # outputs: (n_head*batch_size, len_q, d_v)\n",
    "        # attns: (n_head*batch_size, len_q, len_k)\n",
    "        \n",
    "        # 各ヘッドの結果を連結(上図左側`Concat`)\n",
    "        # torch.splitでbatch_sizeごとのn_head個のテンソルに分割\n",
    "        outputs = torch.split(outputs, batch_size, dim=0)  # (batch_size, len_q, d_v) * n_head\n",
    "        # dim=-1で連結\n",
    "        outputs = torch.cat(outputs, dim=-1)  # (batch_size, len_q, n_head*d_v)\n",
    "\n",
    "        # residual connectionのために元の大きさに写像(上図左側`Linear`)\n",
    "        outputs = self.proj(outputs)  # (batch_size, len_q, d_model)\n",
    "        outputs = self.dropout(outputs)\n",
    "        outputs = self.layer_norm(outputs + residual)\n",
    "\n",
    "        return outputs, attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ③ Position-Wise Feed Forward Network\n",
    "単語列の位置ごとに独立して処理する2層のネットワークであるPosition-Wise Feed Forward Networkを定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    :param d_hid: int, 隠れ層1層目の次元数\n",
    "    :param d_inner_hid: int, 隠れ層2層目の次元数\n",
    "    :param dropout: float, ドロップアウト率\n",
    "    \"\"\"\n",
    "    def __init__(self, d_hid, d_inner_hid, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Conv1d(d_hid, d_inner_hid, 1)\n",
    "        self.w_2 = nn.Conv1d(d_inner_hid, d_hid, 1)\n",
    "        self.layer_norm = nn.LayerNorm(d_hid)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: torch.tensor,\n",
    "            size=(batch_size, max_length, d_hid)\n",
    "        :return: torch.tensor,\n",
    "            size=(batch_size, max_length, d_hid) \n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        output = self.relu(self.w_1(x.transpose(1, 2)))\n",
    "        output = self.w_2(output).transpose(2, 1)\n",
    "        output = self.dropout(output)\n",
    "        return self.layer_norm(output + residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ④ Masking\n",
    "TransfomerではAttentionに対して2つのマスクを定義します。\n",
    "\n",
    "一つはkey側の系列のPADトークンに対してAttentionを行わないようにするマスクです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_attn_padding_mask(seq_q, seq_k):\n",
    "    \"\"\"\n",
    "    keyのPADに対するattentionを0にするためのマスクを作成する\n",
    "    :param seq_q: tensor, queryの系列, size=(batch_size, len_q)\n",
    "    :param seq_k: tensor, keyの系列, size=(batch_size, len_k)\n",
    "    :return pad_attn_mask: tensor, size=(batch_size, len_q, len_k)\n",
    "    \"\"\"\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    pad_attn_mask = seq_k.data.eq(PAD).unsqueeze(1)   # (N, 1, len_k)\n",
    "    pad_attn_mask = pad_attn_mask.expand(batch_size, len_q, len_k) # (N, len_q, len_k)\n",
    "    return pad_attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_seq_q = torch.tensor([[1, 2, 3]])\n",
    "_seq_k = torch.tensor([[4, 5, 6, 7, PAD]])\n",
    "_mask = get_attn_padding_mask(_seq_q, _seq_k)  # 行がquery、列がkeyに対応し、key側がPAD(=0)の時刻だけ1で他が0の行列ができる\n",
    "print('query:\\n', _seq_q)\n",
    "print('key:\\n', _seq_k)\n",
    "print('mask:\\n', _mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "もう一つはDecoder側でSelf Attentionを行う際に、各時刻で未来の情報に対するAttentionを行わないようにするマスクです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_attn_subsequent_mask(seq):\n",
    "    \"\"\"\n",
    "    未来の情報に対するattentionを0にするためのマスクを作成する\n",
    "    :param seq: tensor, size=(batch_size, length)\n",
    "    :return subsequent_mask: tensor, size=(batch_size, length, length)\n",
    "    \"\"\"\n",
    "    attn_shape = (seq.size(1), seq.size(1))\n",
    "    # 上三角行列(diagonal=1: 対角線より上が1で下が0)\n",
    "    subsequent_mask = torch.triu(torch.ones(attn_shape, dtype=torch.uint8, device=device), diagonal=1)\n",
    "    subsequent_mask = subsequent_mask.repeat(seq.size(0), 1, 1)\n",
    "    return subsequent_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iLectで実行する場合warning (GPU is too old) が出ますが, 実行に問題はないので気にせず進めてください.\n",
    "_seq = torch.tensor([[1,2,3,4]])\n",
    "_mask = get_attn_subsequent_mask(_seq)  # 行がquery、列がkeyに対応し、queryより未来のkeyの値が1で他は0の行列ができいる\n",
    "print('seq:\\n', _seq)\n",
    "print('mask:\\n', _mask)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. モデルの定義\n",
    "\n",
    "### Encoder\n",
    "これまで定義してきたサブレイヤーを統合して、Encoderを定義します。\n",
    "\n",
    "EncoderではSelf AttentionとPosition-Wise Feed Forward Networkからなるブロックを複数層繰り返すので、ブロックのクラスEncoderLayerを定義した後にEncoderを定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"Encoderのブロックのクラス\"\"\"\n",
    "    def __init__(self, d_model, d_inner_hid, n_head, d_k, d_v, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param d_model: int, 隠れ層の次元数\n",
    "        :param d_inner_hid: int, Position Wise Feed Forward Networkの隠れ層2層目の次元数\n",
    "        :param n_head: int,　ヘッド数\n",
    "        :param d_k: int, keyベクトルの次元数\n",
    "        :param d_v: int, valueベクトルの次元数\n",
    "        :param dropout: float, ドロップアウト率\n",
    "        \"\"\"\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        # Encoder内のSelf-Attention\n",
    "        self.slf_attn = MultiHeadAttention(\n",
    "            n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        # Postionwise FFN\n",
    "        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner_hid, dropout=dropout)\n",
    "\n",
    "    def forward(self, enc_input, slf_attn_mask=None):\n",
    "        \"\"\"\n",
    "        :param enc_input: tensor, Encoderの入力, \n",
    "            size=(batch_size, max_length, d_model)\n",
    "        :param slf_attn_mask: tensor, Self Attentionの行列にかけるマスク, \n",
    "            size=(batch_size, len_q, len_k)\n",
    "        :return enc_output: tensor, Encoderの出力, \n",
    "            size=(batch_size, max_length, d_model)\n",
    "        :return enc_slf_attn: tensor, EncoderのSelf Attentionの行列, \n",
    "            size=(n_head*batch_size, len_q, len_k)\n",
    "        \"\"\"\n",
    "        # Self-Attentionのquery, key, valueにはすべてEncoderの入力（enc_input）が入る\n",
    "        enc_output, enc_slf_attn = self.slf_attn(\n",
    "            enc_input, enc_input, enc_input, attn_mask=slf_attn_mask)\n",
    "        enc_output = self.pos_ffn(enc_output)\n",
    "        return enc_output, enc_slf_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"EncoderLayerブロックからなるEncoderのクラス\"\"\"\n",
    "    def __init__(\n",
    "            self, n_src_vocab, max_length, n_layers=6, n_head=8, d_k=64, d_v=64,\n",
    "            d_word_vec=512, d_model=512, d_inner_hid=1024, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param n_src_vocab: int, 入力言語の語彙数\n",
    "        :param max_length: int, 最大系列長\n",
    "        :param n_layers: int, レイヤー数\n",
    "        :param n_head: int,　ヘッド数\n",
    "        :param d_k: int, keyベクトルの次元数\n",
    "        :param d_v: int, valueベクトルの次元数\n",
    "        :param d_word_vec: int, 単語の埋め込みの次元数\n",
    "        :param d_model: int, 隠れ層の次元数\n",
    "        :param d_inner_hid: int, Position Wise Feed Forward Networkの隠れ層2層目の次元数\n",
    "        :param dropout: float, ドロップアウト率        \n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        n_position = max_length + 1\n",
    "        self.max_length = max_length\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Positional Encodingを用いたEmbedding\n",
    "        self.position_enc = nn.Embedding(n_position, d_word_vec, padding_idx=PAD)\n",
    "        self.position_enc.weight.data = position_encoding_init(n_position, d_word_vec)\n",
    "\n",
    "        # 一般的なEmbedding\n",
    "        self.src_word_emb = nn.Embedding(n_src_vocab, d_word_vec, padding_idx=PAD)\n",
    "\n",
    "        # EncoderLayerをn_layers個積み重ねる\n",
    "        self.layer_stack = nn.ModuleList([\n",
    "            EncoderLayer(d_model, d_inner_hid, n_head, d_k, d_v, dropout=dropout)\n",
    "            for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, src_seq, src_pos):\n",
    "        \"\"\"\n",
    "        :param src_seq: tensor, 入力系列, \n",
    "            size=(batch_size, max_length)\n",
    "        :param src_pos: tensor, 入力系列の各単語の位置情報,\n",
    "            size=(batch_size, max_length)\n",
    "        :return enc_output: tensor, Encoderの最終出力, \n",
    "            size=(batch_size, max_length, d_model)\n",
    "        :return enc_slf_attns: list, EncoderのSelf Attentionの行列のリスト\n",
    "        \"\"\"\n",
    "        # 一般的な単語のEmbeddingを行う\n",
    "        enc_input = self.src_word_emb(src_seq)\n",
    "        # Positional EncodingのEmbeddingを加算する\n",
    "        enc_input += self.position_enc(src_pos)\n",
    "\n",
    "        enc_slf_attns = []\n",
    "        enc_output = enc_input\n",
    "        # key(=enc_input)のPADに対応する部分のみ1のマスクを作成\n",
    "        enc_slf_attn_mask = get_attn_padding_mask(src_seq, src_seq)\n",
    "\n",
    "        # n_layers個のEncoderLayerに入力を通す\n",
    "        for enc_layer in self.layer_stack:\n",
    "            enc_output, enc_slf_attn = enc_layer(\n",
    "                enc_output, slf_attn_mask=enc_slf_attn_mask)\n",
    "            enc_slf_attns += [enc_slf_attn]\n",
    "\n",
    "        return enc_output, enc_slf_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "Deocoderも同様にSelf Attention, Source-Target Attention, Position-Wise Feed Forward Networkからなるブロックを複数層繰り返ので、ブロックのクラスDecoderLayerを定義した後にDecoderを定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"Decoderのブロックのクラス\"\"\"\n",
    "    def __init__(self, d_model, d_inner_hid, n_head, d_k, d_v, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param d_model: int, 隠れ層の次元数\n",
    "        :param d_inner_hid: int, Position Wise Feed Forward Networkの隠れ層2層目の次元数\n",
    "        :param n_head: int,　ヘッド数\n",
    "        :param d_k: int, keyベクトルの次元数\n",
    "        :param d_v: int, valueベクトルの次元数\n",
    "        :param dropout: float, ドロップアウト率\n",
    "        \"\"\"\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        # Decoder内のSelf-Attention\n",
    "        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        # Encoder-Decoder間のSource-Target Attention\n",
    "        self.enc_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        # Positionwise FFN\n",
    "        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner_hid, dropout=dropout)\n",
    "\n",
    "    def forward(self, dec_input, enc_output, slf_attn_mask=None, dec_enc_attn_mask=None):\n",
    "        \"\"\"\n",
    "        :param dec_input: tensor, Decoderの入力, \n",
    "            size=(batch_size, max_length, d_model)\n",
    "        :param enc_output: tensor, Encoderの出力, \n",
    "            size=(batch_size, max_length, d_model)\n",
    "        :param slf_attn_mask: tensor, Self Attentionの行列にかけるマスク, \n",
    "            size=(batch_size, len_q, len_k)\n",
    "        :param dec_enc_attn_mask: tensor, Soutce-Target Attentionの行列にかけるマスク, \n",
    "            size=(batch_size, len_q, len_k)\n",
    "        :return dec_output: tensor, Decoderの出力, \n",
    "            size=(batch_size, max_length, d_model)\n",
    "        :return dec_slf_attn: tensor, DecoderのSelf Attentionの行列, \n",
    "            size=(n_head*batch_size, len_q, len_k)\n",
    "        :return dec_enc_attn: tensor, DecoderのSoutce-Target Attentionの行列, \n",
    "            size=(n_head*batch_size, len_q, len_k)\n",
    "        \"\"\"\n",
    "        # Self-Attentionのquery, key, valueにはすべてDecoderの入力（dec_input）が入る\n",
    "        dec_output, dec_slf_attn = # WRITE ME!\n",
    "        \n",
    "        # Source-Target AttentionのqueryにはDecoderの出力(dec_output), key, valueにはEncoderの出力（enc_output）が入る\n",
    "        dec_output, dec_enc_attn = # WRITE ME!\n",
    "        \n",
    "        # Positionwise FFNに通す\n",
    "        dec_output = # WRITE ME!\n",
    "\n",
    "        return dec_output, dec_slf_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"DecoderLayerブロックからなるDecoderのクラス\"\"\"\n",
    "    def __init__(\n",
    "            self, n_tgt_vocab, max_length, n_layers=6, n_head=8, d_k=64, d_v=64,\n",
    "            d_word_vec=512, d_model=512, d_inner_hid=1024, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param n_tgt_vocab: int, 出力言語の語彙数\n",
    "        :param max_length: int, 最大系列長\n",
    "        :param n_layers: int, レイヤー数\n",
    "        :param n_head: int,　ヘッド数\n",
    "        :param d_k: int, keyベクトルの次元数\n",
    "        :param d_v: int, valueベクトルの次元数\n",
    "        :param d_word_vec: int, 単語の埋め込みの次元数\n",
    "        :param d_model: int, 隠れ層の次元数\n",
    "        :param d_inner_hid: int, Position Wise Feed Forward Networkの隠れ層2層目の次元数\n",
    "        :param dropout: float, ドロップアウト率        \n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        n_position = max_length + 1\n",
    "        self.max_length = max_length\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Positional Encodingを用いたEmbedding\n",
    "        self.position_enc = nn.Embedding(\n",
    "            n_position, d_word_vec, padding_idx=PAD)\n",
    "        self.position_enc.weight.data = position_encoding_init(n_position, d_word_vec)\n",
    "\n",
    "        # 一般的なEmbedding\n",
    "        self.tgt_word_emb = nn.Embedding(\n",
    "            n_tgt_vocab, d_word_vec, padding_idx=PAD)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # DecoderLayerをn_layers個積み重ねる\n",
    "        self.layer_stack = nn.ModuleList([\n",
    "            DecoderLayer(d_model, d_inner_hid, n_head, d_k, d_v, dropout=dropout)\n",
    "            for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, tgt_seq, tgt_pos, src_seq, enc_output):\n",
    "        \"\"\"\n",
    "        :param tgt_seq: tensor, 出力系列, \n",
    "            size=(batch_size, max_length)\n",
    "        :param tgt_pos: tensor, 出力系列の各単語の位置情報,\n",
    "            size=(batch_size, max_length)\n",
    "        :param src_seq: tensor, 入力系列, \n",
    "            size=(batch_size, n_src_vocab)\n",
    "        :param enc_output: tensor, Encoderの出力, \n",
    "            size=(batch_size, max_length, d_model)\n",
    "        :return dec_output: tensor, Decoderの最終出力, \n",
    "            size=(batch_size, max_length, d_model)\n",
    "        :return dec_slf_attns: list, DecoderのSelf Attentionの行列のリスト \n",
    "        :return dec_slf_attns: list, DecoderのSelf Attentionの行列のリスト\n",
    "        \"\"\"\n",
    "        # 一般的な単語のEmbeddingを行う\n",
    "        dec_input = self.tgt_word_emb(tgt_seq)\n",
    "        # Positional EncodingのEmbeddingを加算する\n",
    "        dec_input += self.position_enc(tgt_pos)\n",
    "\n",
    "        # Self-Attention用のマスクを作成\n",
    "        # key(=dec_input)のPADに対応する部分が1のマスクと、queryから見たkeyの未来の情報に対応する部分が1のマスクのORをとる\n",
    "        dec_slf_attn_pad_mask = get_attn_padding_mask(tgt_seq, tgt_seq)  # (N, max_length, max_length)\n",
    "        dec_slf_attn_sub_mask = get_attn_subsequent_mask(tgt_seq)  # (N, max_length, max_length)\n",
    "        dec_slf_attn_mask = torch.gt(dec_slf_attn_pad_mask + dec_slf_attn_sub_mask, 0)  # ORをとる\n",
    "\n",
    "        # key(=dec_input)のPADに対応する部分のみ1のマスクを作成\n",
    "        dec_enc_attn_pad_mask = get_attn_padding_mask(tgt_seq, src_seq)  # (N, max_length, max_length)\n",
    "\n",
    "        dec_slf_attns, dec_enc_attns = [], []\n",
    "\n",
    "        dec_output = dec_input\n",
    "        # n_layers個のDecoderLayerに入力を通す\n",
    "        for dec_layer in self.layer_stack:\n",
    "            dec_output, dec_slf_attn, dec_enc_attn = dec_layer(\n",
    "                dec_output, enc_output,\n",
    "                slf_attn_mask=dec_slf_attn_mask,\n",
    "                dec_enc_attn_mask=dec_enc_attn_pad_mask)\n",
    "\n",
    "            dec_slf_attns += [dec_slf_attn]\n",
    "            dec_enc_attns += [dec_enc_attn]\n",
    "\n",
    "        return dec_output, dec_slf_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer\n",
    "最後にモデル全体の機構を定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"Transformerのモデル全体のクラス\"\"\"\n",
    "    def __init__(\n",
    "            self, n_src_vocab, n_tgt_vocab, max_length, n_layers=6, n_head=8,\n",
    "            d_word_vec=512, d_model=512, d_inner_hid=1024, d_k=64, d_v=64,\n",
    "            dropout=0.1, proj_share_weight=True):\n",
    "        \"\"\"\n",
    "        :param n_src_vocab: int, 入力言語の語彙数\n",
    "        :param n_tgt_vocab: int, 出力言語の語彙数\n",
    "        :param max_length: int, 最大系列長\n",
    "        :param n_layers: int, レイヤー数\n",
    "        :param n_head: int,　ヘッド数\n",
    "        :param d_k: int, keyベクトルの次元数\n",
    "        :param d_v: int, valueベクトルの次元数\n",
    "        :param d_word_vec: int, 単語の埋め込みの次元数\n",
    "        :param d_model: int, 隠れ層の次元数\n",
    "        :param d_inner_hid: int, Position Wise Feed Forward Networkの隠れ層2層目の次元数\n",
    "        :param dropout: float, ドロップアウト率        \n",
    "        :param proj_share_weight: bool, 出力言語の単語のEmbeddingと出力の写像で重みを共有する        \n",
    "        \"\"\"\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(\n",
    "            n_src_vocab, max_length, n_layers=n_layers, n_head=n_head,\n",
    "            d_word_vec=d_word_vec, d_model=d_model,\n",
    "            d_inner_hid=d_inner_hid, dropout=dropout)\n",
    "        self.decoder = Decoder(\n",
    "            n_tgt_vocab, max_length, n_layers=n_layers, n_head=n_head,\n",
    "            d_word_vec=d_word_vec, d_model=d_model,\n",
    "            d_inner_hid=d_inner_hid, dropout=dropout)\n",
    "        self.tgt_word_proj = nn.Linear(d_model, n_tgt_vocab, bias=False)\n",
    "        nn.init.xavier_normal_(self.tgt_word_proj.weight)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        assert d_model == d_word_vec  # 各モジュールの出力のサイズは揃える\n",
    "\n",
    "        if proj_share_weight:\n",
    "            # 出力言語の単語のEmbeddingと出力の写像で重みを共有する\n",
    "            assert d_model == d_word_vec\n",
    "            self.tgt_word_proj.weight = self.decoder.tgt_word_emb.weight\n",
    "\n",
    "    def get_trainable_parameters(self):\n",
    "        # Positional Encoding以外のパラメータを更新する\n",
    "        enc_freezed_param_ids = set(map(id, self.encoder.position_enc.parameters()))\n",
    "        dec_freezed_param_ids = set(map(id, self.decoder.position_enc.parameters()))\n",
    "        freezed_param_ids = enc_freezed_param_ids | dec_freezed_param_ids\n",
    "        return (p for p in self.parameters() if id(p) not in freezed_param_ids)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_seq, src_pos = src\n",
    "        tgt_seq, tgt_pos = tgt\n",
    "\n",
    "        src_seq = src_seq[:, 1:]\n",
    "        src_pos = src_pos[:, 1:]\n",
    "        tgt_seq = tgt_seq[:, :-1]\n",
    "        tgt_pos = tgt_pos[:, :-1]\n",
    "\n",
    "        enc_output, *_ = self.encoder(src_seq, src_pos)\n",
    "        dec_output, *_ = self.decoder(tgt_seq, tgt_pos, src_seq, enc_output)\n",
    "        seq_logit = self.tgt_word_proj(dec_output)\n",
    "\n",
    "        return seq_logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss(batch_X, batch_Y, model, criterion, optimizer=None, is_train=True):\n",
    "    # バッチの損失を計算\n",
    "    model.train(is_train)\n",
    "    \n",
    "    pred_Y = model(batch_X, batch_Y)\n",
    "    gold = batch_Y[0][:, 1:].contiguous()\n",
    "    loss = criterion(pred_Y.view(-1, pred_Y.size(2)), gold.view(-1))\n",
    "\n",
    "    if is_train:  # 訓練時はパラメータを更新\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    gold = gold.data.cpu().numpy().tolist()\n",
    "    pred = pred_Y.max(dim=-1)[1].data.cpu().numpy().tolist()\n",
    "\n",
    "    return loss.item(), gold, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 20\n",
    "batch_size = 64\n",
    "num_epochs = 15\n",
    "lr = 0.001\n",
    "ckpt_path = 'transformer.pth'\n",
    "max_length = MAX_LENGTH + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_args = {\n",
    "    'n_src_vocab': vocab_size_X,\n",
    "    'n_tgt_vocab': vocab_size_Y,\n",
    "    'max_length': max_length,\n",
    "    'proj_share_weight': True,\n",
    "    'd_k': 32,\n",
    "    'd_v': 32,\n",
    "    'd_model': 128,\n",
    "    'd_word_vec': 128,\n",
    "    'd_inner_hid': 256,\n",
    "    'n_layers': 3,\n",
    "    'n_head': 6,\n",
    "    'dropout': 0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DataLoaderやモデルを定義\n",
    "train_dataloader = DataLoader(\n",
    "    train_X, train_Y, batch_size\n",
    "    )\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_X, valid_Y, batch_size, \n",
    "    shuffle=False\n",
    "    )\n",
    "\n",
    "model = Transformer(**model_args).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.get_trainable_parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD, reduction='sum').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_bleu(refs, hyps):\n",
    "    \"\"\"\n",
    "    BLEUスコアを計算する関数\n",
    "    :param refs: list, 参照訳。単語のリストのリスト (例： [['I', 'have', 'a', 'pen'], ...])\n",
    "    :param hyps: list, モデルの生成した訳。単語のリストのリスト (例： [['I', 'have', 'a', 'pen'], ...])\n",
    "    :return: float, BLEUスコア(0~100)\n",
    "    \"\"\"\n",
    "    refs = [[ref[:ref.index(EOS)]] for ref in refs]\n",
    "    hyps = [hyp[:hyp.index(EOS)] if EOS in hyp else hyp for hyp in hyps]\n",
    "    return 100 * bleu_score.corpus_bleu(refs, hyps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練\n",
    "best_valid_bleu = 0.\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    start = time.time()\n",
    "    train_loss = 0.\n",
    "    train_refs = []\n",
    "    train_hyps = []\n",
    "    valid_loss = 0.\n",
    "    valid_refs = []\n",
    "    valid_hyps = []\n",
    "    # train\n",
    "    for batch in train_dataloader:\n",
    "        batch_X, batch_Y = batch\n",
    "        loss, gold, pred = compute_loss(\n",
    "            batch_X, batch_Y, model, criterion, optimizer, is_train=True\n",
    "            )\n",
    "        train_loss += loss\n",
    "        train_refs += gold\n",
    "        train_hyps += pred\n",
    "    # valid\n",
    "    for batch in valid_dataloader:\n",
    "        batch_X, batch_Y = batch\n",
    "        loss, gold, pred = compute_loss(\n",
    "            batch_X, batch_Y, model, criterion, is_train=False\n",
    "            )\n",
    "        valid_loss += loss\n",
    "        valid_refs += gold\n",
    "        valid_hyps += pred\n",
    "    # 損失をサンプル数で割って正規化\n",
    "    train_loss /= len(train_dataloader.data) \n",
    "    valid_loss /= len(valid_dataloader.data) \n",
    "    # BLEUを計算\n",
    "    train_bleu = calc_bleu(train_refs, train_hyps)\n",
    "    valid_bleu = calc_bleu(valid_refs, valid_hyps)\n",
    "\n",
    "    # validationデータでBLEUが改善した場合にはモデルを保存\n",
    "    if valid_bleu > best_valid_bleu:\n",
    "        ckpt = model.state_dict()\n",
    "        torch.save(ckpt, ckpt_path)\n",
    "        best_valid_bleu = valid_bleu\n",
    "\n",
    "    elapsed_time = (time.time()-start) / 60\n",
    "    print('Epoch {} [{:.1f}min]: train_loss: {:5.2f}  train_bleu: {:2.2f}  valid_loss: {:5.2f}  valid_bleu: {:2.2f}'.format(\n",
    "            epoch, elapsed_time, train_loss, train_bleu, valid_loss, valid_bleu))\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(model, src, max_length=20):\n",
    "    # 学習済みモデルで系列を生成する\n",
    "    model.eval()\n",
    "    \n",
    "    src_seq, src_pos = src\n",
    "    batch_size = src_seq.size(0)\n",
    "    enc_output, enc_slf_attns = model.encoder(src_seq, src_pos)\n",
    "        \n",
    "    tgt_seq = torch.full([batch_size, 1], BOS, dtype=torch.long, device=device)\n",
    "    tgt_pos = torch.arange(1, dtype=torch.long, device=device)\n",
    "    tgt_pos = tgt_pos.unsqueeze(0).repeat(batch_size, 1)\n",
    "\n",
    "    # 時刻ごとに処理\n",
    "    for t in range(1, max_length+1):\n",
    "        dec_output, dec_slf_attns, dec_enc_attns = model.decoder(\n",
    "            tgt_seq, tgt_pos, src_seq, enc_output)\n",
    "        dec_output = model.tgt_word_proj(dec_output)\n",
    "        out = dec_output[:, -1, :].max(dim=-1)[1].unsqueeze(1)\n",
    "        # 自身の出力を次の時刻の入力にする\n",
    "        tgt_seq = torch.cat([tgt_seq, out], dim=-1)\n",
    "        tgt_pos = torch.arange(t+1, dtype=torch.long, device=device)\n",
    "        tgt_pos = tgt_pos.unsqueeze(0).repeat(batch_size, 1)\n",
    "\n",
    "    return tgt_seq[:, 1:], enc_slf_attns, dec_slf_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ids_to_sentence(vocab, ids):\n",
    "    # IDのリストを単語のリストに変換する\n",
    "    return [vocab.id2word[_id] for _id in ids]\n",
    "\n",
    "def trim_eos(ids):\n",
    "    # IDのリストからEOS以降の単語を除外する\n",
    "    if EOS in ids:\n",
    "        return ids[:ids.index(EOS)]\n",
    "    else:\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 学習済みモデルの読み込み\n",
    "model = Transformer(**model_args).to(device)\n",
    "ckpt = torch.load(ckpt_path)\n",
    "model.load_state_dict(ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# テストデータの読み込み\n",
    "test_X = load_data('../data/dev.en')\n",
    "test_Y = load_data('../data/dev.ja')\n",
    "test_X = [sentence_to_ids(vocab_X, sentence) for sentence in test_X]\n",
    "test_Y = [sentence_to_ids(vocab_Y, sentence) for sentence in test_Y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(\n",
    "    test_X, test_Y, 1,\n",
    "    shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "src, tgt = next(test_dataloader)\n",
    "\n",
    "src_ids = src[0][0].cpu().numpy()\n",
    "tgt_ids = tgt[0][0].cpu().numpy()\n",
    "\n",
    "print('src: {}'.format(' '.join(ids_to_sentence(vocab_X, src_ids[1:-1]))))\n",
    "print('tgt: {}'.format(' '.join(ids_to_sentence(vocab_Y, tgt_ids[1:-1]))))\n",
    "\n",
    "preds, enc_slf_attns, dec_slf_attns, dec_enc_attns = test(model, src)\n",
    "pred_ids = preds[0].data.cpu().numpy().tolist()\n",
    "print('out: {}'.format(' '.join(ids_to_sentence(vocab_Y, trim_eos(pred_ids)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEUの評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# BLEUの評価\n",
    "test_dataloader = DataLoader(\n",
    "    test_X, test_Y, 128,\n",
    "    shuffle=False\n",
    "    )\n",
    "refs_list = []\n",
    "hyp_list = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    batch_X, batch_Y = batch\n",
    "    preds, *_ = test(model, batch_X)\n",
    "    preds = preds.data.cpu().numpy().tolist()\n",
    "    refs = batch_Y[0].data.cpu().numpy()[:, 1:].tolist()\n",
    "    refs_list += refs\n",
    "    hyp_list += preds\n",
    "bleu = calc_bleu(refs_list, hyp_list)\n",
    "print(bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Attentionの可視化\n",
    "\n",
    "学習済みモデルのAttentionを可視化します。\n",
    "\n",
    "以下は元論文の図で、Encoder側のあるレイヤーにおけるSelf-Attentionを可視化したものです。\n",
    "\n",
    "異なる色が異なるヘッドを、色の濃淡がAttentionの重みを表しており、各ヘッドが異なる位置へのAttentionを担っていることがわかります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/AttentionVisualization.png\" style=\"height:400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下ではもう少し単純に、各行をquery、各列をkeyとしてAttentionの行列を可視化します。\n",
    "\n",
    "モデルはEncoder/Decoderそれぞれが複数のレイヤーを有し、各レイヤー内の各Attentionも複数のヘッドを有するので、順番に中身を可視化していきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_attentions(attention, sent_q, sent_k, n_max=10):\n",
    "    # Attentionの行列を可視化する(n_maxは表示するヘッドの最大数)\n",
    "    # EOS以降を削除\n",
    "    q_eos = sent_q.index(EOS_TOKEN)\n",
    "    k_eos = sent_k.index(EOS_TOKEN)\n",
    "    sent_q = sent_q[:q_eos+1]\n",
    "    sent_k = sent_k[:k_eos+1]\n",
    "    \n",
    "    n_head = attention.size(0)\n",
    "    n_head = min(n_head, n_max)\n",
    "    fig, axes = plt.subplots(1, n_head, figsize=(20, 8))\n",
    "    for h in range(n_head):\n",
    "        data = attention[h].cpu().data[:q_eos+1, :k_eos+1]\n",
    "        x = sent_k\n",
    "        y = sent_q if h == 0 else []\n",
    "        sns.heatmap(data, vmin=0., vmax=1., cbar=False, ax=axes[h], cmap='Blues')\n",
    "        axes[h].set_xticklabels(x)\n",
    "        axes[h].set_yticklabels(y, rotation=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_context('poster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_sent = [vocab_X.id2word[i] for i in src_ids[1:]]\n",
    "pred_sent = [vocab_Y.id2word[i] for i in pred_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 1st layer\n",
    "show_attentions(enc_slf_attns[0], src_sent, src_sent, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 2nd layer\n",
    "show_attentions(enc_slf_attns[1], src_sent, src_sent, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 3rd layer\n",
    "show_attentions(enc_slf_attns[2], src_sent, src_sent, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 1st layer\n",
    "show_attentions(dec_slf_attns[0], pred_sent, pred_sent, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 2nd layer\n",
    "show_attentions(dec_slf_attns[1], pred_sent, pred_sent, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 3rd layer\n",
    "show_attentions(dec_slf_attns[2], pred_sent, pred_sent, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Decoder Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 1st layer\n",
    "show_attentions(dec_enc_attns[0], pred_sent, src_sent, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 2nd layer\n",
    "show_attentions(dec_enc_attns[1], pred_sent, src_sent, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 3rd layer\n",
    "show_attentions(dec_enc_attns[2], pred_sent, src_sent, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
