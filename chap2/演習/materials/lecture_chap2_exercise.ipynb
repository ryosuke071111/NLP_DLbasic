{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第2回講義　演習　Word2Vec(Skipgram, CBOW)の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vecとは2013年に当時GoogleにいたTomas Mikolovらによって発表された論文に端を発する、単語の分散表現を学習するためのアルゴリズムの総称です。\n",
    "\n",
    "NLPのタスクにおいて入力は単語で構成されていますが、単語を単語のまま扱っていてはコンピュータにとってはただの文字の羅列でしかなく、翻訳や文書分類などの複雑なNLPのタスクを解くことは困難です。\n",
    "\n",
    "そのため単語やフレーズ、文などの意味を持つ要素をいかに有用な意味表現ないし特徴量に変換できるかがタスクのパフォーマンスに直結します。\n",
    "\n",
    "Word2Vecは単語から有用な特徴量（ベクトル）を学習することができる最も有名なツールの１つです。Word2Vecを使うと意味的に似たような単語は似たようなベクトルに変換することができ、さらには得られた単語のベクトルが加法性を持つという特徴があります。例えば、\"Japan\" - \"Tokyo\" ≒ \"France\" - \"Paris\"など単語を使った足し算・引き算を擬似的に行えます。\n",
    "\n",
    "下図のグラフは、Word2Vecのアルゴリズムの1つであるSkipgramで単語のベクトルを獲得した後にPCAを使って次元圧縮をしたのちにいくつかの国名とその首都のベクトルを可視化したものです。教師なし学習であるにも関わらず、このような国と首都の関係まで学習できてしまうのがWord2Vecが有名になった要因の一つでもあります。\n",
    "\n",
    "<img src=../image/country-city.png width=500>\n",
    "<div style=\"text-align: center;\">\n",
    "出典:[Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)\n",
    "</div>\n",
    "\n",
    "Word2vecの学習モデルは大きく分けて2種類あり、CBOWとSkipgramと呼ばれています。\n",
    "\n",
    "CBOWは周囲の単語を元に単語を予測するタスク、Skipgramは周囲の単語を予測するタスクを解けるような単語のベクトルを学習します。\n",
    "\n",
    "これらはともに単語は周囲の単語によって特徴づけられるという\"分布仮説\"(distributional hypothesis)に基づいていると捉えることができます。\n",
    "\n",
    "この演習ではCBOWとSkipgramをPyTorchを使って実装し、実際に日本語のデータセットで学習します。(そのため日本語の前処理も扱います。)さらに、効率的な学習方法であるNegative Samplingも実装します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# 必要であれば変更してください\n",
    "import os\n",
    "os.chdir('/root/userspace/chap2/materials')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. 形態素解析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文などを分割して、言語で意味を持つ最小単位である形態素の列に変換し、その品詞を推定することを形態素解析と呼びます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "坊主\tボウズ\t坊主\t名詞-一般\t\t\n",
      "が\tガ\tが\t助詞-格助詞-一般\t\t\n",
      "屏風\tビョウブ\t屏風\t名詞-一般\t\t\n",
      "に\tニ\tに\t助詞-格助詞-一般\t\t\n",
      "上手\tジョウズ\t上手\t名詞-形容動詞語幹\t\t\n",
      "に\tニ\tに\t助詞-副詞化\t\t\n",
      "坊主\tボウズ\t坊主\t名詞-一般\t\t\n",
      "の\tノ\tの\t助詞-連体化\t\t\n",
      "絵\tエ\t絵\t名詞-一般\t\t\n",
      "を\tヲ\tを\t助詞-格助詞-一般\t\t\n",
      "描い\tエガイ\t描く\t動詞-自立\t五段・カ行イ音便\t連用タ接続\n",
      "た\tタ\tた\t助動詞\t特殊・タ\t基本形\n",
      "EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tagger = MeCab.Tagger(\"-Ochasen\")\n",
    "node = tagger.parse(\"坊主が屏風に上手に坊主の絵を描いた\")\n",
    "print(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. MeCabを用いて文を形態素に分割"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先ほどの形態素解析の結果を用いて、日本語の文を形態素に分割する関数を定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    \"\"\"日本語の文を形態素の列に分割する関数\n",
    "\n",
    "    :param sentence: str, 日本語の文\n",
    "    :return tokenized_sentence: list of str, 形態素のリスト\n",
    "    \"\"\"\n",
    "    node = tagger.parse(sentence)\n",
    "    node = node.split(\"\\n\")\n",
    "    tokenized_sentence = []\n",
    "    for i in range(len(node)):\n",
    "        feature = node[i].split(\"\\t\")\n",
    "        if feature[0] == \"EOS\":\n",
    "            # 文が終わったら終了\n",
    "            break\n",
    "        # 分割された形態素を追加\n",
    "        tokenized_sentence.append(feature[0])\n",
    "    return tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['坊主', 'が', '屏風', 'に', '上手', 'に', '坊主', 'の', '絵', 'を', '描い', 'た']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"坊主が屏風に上手に坊主の絵を描いた\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. データ読み込み"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "夏目漱石の『こころ』(../data/kokoro.txt)を題材にします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "私はその人を常に先生と呼んでいた。\r\n",
      "だからここでもただ先生と書くだけで本名は打ち明けない。\r\n",
      "これは世間を憚かる遠慮というよりも、その方が私にとって自然だからである。\r\n",
      "私はその人の記憶を呼び起すごとに、すぐ「先生」といいたくなる。\r\n",
      "筆を執っても心持は同じ事である。\r\n"
     ]
    }
   ],
   "source": [
    "# 頭から5行を表示\n",
    "! head -5 ../data/kokoro.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    \"\"\"『こころ』を読み込むための関数\n",
    "\n",
    "    :param path: str, 『こころ』のパス\n",
    "    :return text: list of list of str, 各文がトークナイズされた『こころ』\n",
    "    \"\"\"\n",
    "    text = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            line = tokenize(line)\n",
    "            text.append(line)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = load_data(\"../data/kokoro.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['私', 'は', 'その', '人', 'を', '常に', '先生', 'と', '呼ん', 'で', 'い', 'た', '。']\n"
     ]
    }
   ],
   "source": [
    "# 分割された結果の例\n",
    "print(text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. 辞書構築\n",
    "コーパス（文書データ）に登場する単語にユニークなIDを振るために辞書を構築します。\n",
    "\n",
    "全ての単語を辞書に登録するとしばしば語彙数が膨大(1万〜100万)になりメモリが足りなくなることがあるので、\n",
    "\n",
    "単語の出現頻度で足切りして辞書のサイズを制限するということをします。\n",
    "\n",
    "NLPではほぼ必ず必要になる手順です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特殊なトークンとそのIDは事前に定義しておきます。\n",
    "PAD_TOKEN = '<PAD>' # あとで説明するpaddingに使います\n",
    "UNK_TOKEN = '<UNK>' # 辞書にない単語は全てこのUNKトークンに置き換えます。(UNKの由来はunkownです)\n",
    "PAD = 0 # <PAD>のID\n",
    "UNK = 1 # <UNK>のID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 辞書の初期化\n",
    "word2id = {\n",
    "    PAD_TOKEN: PAD,\n",
    "    UNK_TOKEN: UNK,\n",
    "}\n",
    "\n",
    "# 辞書に含める単語の最低出現回数\n",
    "# 今回はコーパスのサイズが小さいので、全ての単語を辞書に含めることにします\n",
    "MIN_COUNT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    \"\"\"語彙を管理するためのクラス\"\"\"\n",
    "    def __init__(self, word2id={}):\n",
    "        \"\"\"\n",
    "        :param word2id: 単語(str)をインデックス(int)に変換する辞書\n",
    "        \"\"\"\n",
    "        self.word2id = dict(word2id)\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}    \n",
    "\n",
    "    def build_vocab(self, sentences, min_count=1):\n",
    "        \"\"\"コーパスから語彙の辞書を構築するメソッド\n",
    "\n",
    "        :param sentences: list of list of str, コーパス\n",
    "        :param min_count: int, 辞書に含める単語の最小出現回数\n",
    "        \"\"\"\n",
    "        # 各単語の出現回数をカウントする辞書を作成します\n",
    "        word_counter = {}\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                # dict.get(key, 0)はdictにkeyがあればdict[key]を、なければ0を返すメソッドです\n",
    "                word_counter[word] = word_counter.get(word, 0) + 1\n",
    "\n",
    "        # min_count回以上出現する単語のみ語彙に加えます\n",
    "        # 出現回数の高い単語から順にword2idに追加していきます\n",
    "        # 出現回数に-1をかけた値でsortすることで出現回数の降順になるようにしています\n",
    "        for word, count in sorted(word_counter.items(), key=lambda x: -x[1]):\n",
    "            if count < min_count:\n",
    "                break\n",
    "            _id = len(self.word2id)\n",
    "            self.word2id.setdefault(word, _id)\n",
    "            self.id2word[_id] = word\n",
    "\n",
    "        # 語彙に含まれる単語の出現回数を保持します（あとで使います）\n",
    "        self.raw_vocab = {w: word_counter[w] for w in self.word2id.keys() if w in word_counter}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "語彙数: 6659\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(word2id=word2id)\n",
    "vocab.build_vocab(text, min_count=MIN_COUNT)\n",
    "print(\"語彙数:\", len(vocab.word2id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. 単語のID化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_ids(vocab, sen):\n",
    "    \"\"\"\n",
    "    単語のリストをIDのリストに変換する関数\n",
    "\n",
    "    :param vocab: class `Vocab` object\n",
    "    :param sen: list of str, 文を分かち書きして得られた単語のリスト\n",
    "    :return out: list of int, 単語IDのリスト\n",
    "    \"\"\"\n",
    "    out = [vocab.word2id.get(word, UNK) for word in sen] # 辞書にない単語にはUNKのIDを割り振ります\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 日本語のテキストを単語IDに変換します。\n",
    "id_text = [sentence_to_ids(vocab, sen) for sen in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['私', 'は', 'その', '人', 'を', '常に', '先生', 'と', '呼ん', 'で', 'い', 'た', '。']\n",
      "[10, 5, 23, 39, 9, 689, 26, 12, 485, 13, 19, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "print(text[0])\n",
    "print(id_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. Padding\n",
    "NLPではよくsequence（文や単語の列など）の長さを統一するためにPaddingという操作を行います。\n",
    "\n",
    "各sequenceの長さが揃っていないと行列演算を行えないからです。\n",
    "\n",
    "ここではmax_length以下の長さのsequenceにはPAD(=0)を後ろに必要なだけ追加します。\n",
    "\n",
    "max_lengthを超える長さのsequenceはデータから省くor末尾を必要なだけ削るなどをして対処します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq(seq, max_length):\n",
    "    \"\"\"Paddingを行う関数\n",
    "\n",
    "    :param seq: list of int, 単語のインデックスのリスト\n",
    "    :param max_length: int, バッチ内の系列の最大長\n",
    "    :return seq: list of int, 単語のインデックスのリスト\n",
    "    \"\"\"\n",
    "    seq += [PAD for i in range(max_length - len(seq))]\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CBOW (Continuous Bag of Words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "周囲の単語($w(t-2)...w(t+2)$)からターゲットの単語($w(t)$)を予測するタスクを解くことで単語のベクトルを学習します。\n",
    "\n",
    "まず周囲の単語を埋め込み層でベクトルにしたのち、その和をとってから全結合層を介してターゲットとなる単語を予測します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../image/cbow.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "出典:[Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "batch_size = 64 # ミニバッチのサイズ\n",
    "n_batches = 300 # 今回学習するミニバッチの数\n",
    "vocab_size = len(vocab.word2id) # 語彙の総数\n",
    "embedding_size = 300 # 各単語に割り当てるベクトルの次元数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips: イテレータの使い方"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 データローダーでイテレータを使うのですが、イテレータに馴染みがない方もいるかと思われるので、ここでPythonでのイテレータの簡単な使い方に触れておきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestIter(object):\n",
    "    def __init__(self):\n",
    "        self.iter = 0\n",
    "        self.max_iter = 5\n",
    "    \n",
    "    def __iter__(self): # 必須\n",
    "        print(\"iter関数が呼び出されました\")\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        self.iter += 1\n",
    "        print(\"next関数が呼び出されました({}回目)\".format(self.iter))\n",
    "        if self.iter < self.max_iter:\n",
    "            return None\n",
    "        else:\n",
    "            print(\"max_iterに達したので終了します\")\n",
    "            raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter関数が呼び出されました\n"
     ]
    }
   ],
   "source": [
    "# インスタンス生成\n",
    "testiter = TestIter()\n",
    "# イテレータを生成\n",
    "t = iter(testiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next関数が呼び出されました(1回目)\n",
      "next関数が呼び出されました(2回目)\n"
     ]
    }
   ],
   "source": [
    "next(t)\n",
    "next(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter関数が呼び出されました\n",
      "next関数が呼び出されました(1回目)\n",
      "next関数が呼び出されました(2回目)\n",
      "next関数が呼び出されました(3回目)\n",
      "next関数が呼び出されました(4回目)\n",
      "next関数が呼び出されました(5回目)\n",
      "max_iterに達したので終了します\n"
     ]
    }
   ],
   "source": [
    "testiter = TestIter()\n",
    "for i in testiter:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. データローダー"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットから一部のデータ（ミニバッチ）を取得するデータローダーを定義します。\n",
    "\n",
    "データセットというのはデータ全体のことを指し、ミニバッチはデータセットを小分けにしたデータの集合を指します。\n",
    "データセットはサイズが大きいケースが多いため、一度に全てのデータセットをモデルに与えて学習させるのではなく、ミニバッチという単位でデータを小分けにしてモデルに複数回与えることで学習させます。\n",
    "(参考：バッチ学習、ミニバッチ学習、オンライン学習)\n",
    "\n",
    "通常、教師あり学習では各ミニバッチは入力データと正解データから構成されます。\n",
    "今回は入力データ(batch_X)が周辺単語(のID群)、正解データ(batch_Y)がターゲットの単語(のID)となります。\n",
    "\n",
    "各ミニバッチはサイズが(batch_size, window*2)のTensorになるようにします。\n",
    "この際、先ほど定義したPaddingという操作を行って各データの長さを揃えます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderCBOW(object):\n",
    "    \"\"\"CBOWのデータローダー\"\"\"\n",
    "    def __init__(self, text, batch_size, window=3):\n",
    "        \"\"\"\n",
    "        :param text: list of list of int, 単語をIDに変換したデータセット\n",
    "        :param batch_size: int, ミニバッチのサイズ\n",
    "        :param window: int, 周辺単語とターゲットの単語の最大距離\n",
    "        \"\"\"\n",
    "        self.text = text\n",
    "        self.batch_size = batch_size\n",
    "        self.window = window\n",
    "        self.s_pointer = 0 # データセット上を走査する文単位のポインタ\n",
    "        self.w_pointer = 0 # データセット上を走査する単語単位のポインタ\n",
    "        self.max_s_pointer = len(text) # データセットに含まれる文の総数\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        batch_X = []\n",
    "        batch_Y = []\n",
    "        while len(batch_X) < self.batch_size:\n",
    "            # 走査する対象の文\n",
    "            sen = self.text[self.s_pointer]\n",
    "            \n",
    "            # 予測すべき単語\n",
    "            word_Y = sen[self.w_pointer]\n",
    "            \n",
    "            # 入力となる単語群を取得\n",
    "            start = max(0, self.w_pointer - self.window)\n",
    "            word_X = sen[start:self.w_pointer] + \\\n",
    "                sen[self.w_pointer + 1:self.w_pointer + self.window + 1]\n",
    "            word_X = pad_seq(word_X, self.window * 2)\n",
    "            \n",
    "            batch_X.append(word_X)\n",
    "            batch_Y.append(word_Y)\n",
    "            self.w_pointer += 1\n",
    "            \n",
    "            if self.w_pointer >= len(sen):\n",
    "                # 文を走査し終わったら次の文の先頭にポインタを移行する\n",
    "                self.w_pointer = 0\n",
    "                self.s_pointer += 1\n",
    "                if self.s_pointer >= self.max_s_pointer:\n",
    "                    # 全ての文を走査し終わったら終了する\n",
    "                    self.s_pointer = 0\n",
    "                    raise StopIteration\n",
    "\n",
    "        # データはtorch.Tensorにする必要があります。dtype, deviceも指定します。\n",
    "        batch_X = torch.tensor(batch_X, dtype=torch.long, device=device)\n",
    "        batch_Y = torch.tensor(batch_Y, dtype=torch.long, device=device)\n",
    "\n",
    "        return batch_X, batch_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips: Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EmbeddingとはNLPでDeep Learningをするときにほぼ必ず使われる手法で、Embeddingを行うLayerをEmbedding Layer（埋め込み層）と言います。\n",
    "\n",
    "端的に言うとEmbeddingとは「ある単語」を（に）「d次元のベクトル」に埋め込む（を割り当てる）ことを指します。\n",
    "\n",
    "語彙に含まれるすべての単語に対応する行ベクトルを縦方向につなげた（語彙数 × ベクトルの次元）の2次行列がEmbedding Layer（埋め込み層）のパラメータで、Embedding Matrixとも呼ばれます。\n",
    "\n",
    "この埋め込み層では入力が単語のID（りんご：13）、出力が入力の単語IDに対応するd次元ベクトル（Embedding Matrixの13行目のベクトル）です。\n",
    "\n",
    "そしてWord2Vecでは学習が終わった後のEmbedding Layerのパラメータを単語表現として扱います。\n",
    "\n",
    "* 用語まとめ\n",
    "    * Embedding\n",
    "        * 単語をベクトルに埋め込むこと\n",
    "    * Embedding Layer\n",
    "        * Embeddingを行うLayer\n",
    "    * Embedding Matrix\n",
    "        * Embedding Layerのパラメータで、サイズは(語彙数, ベクトルの次元)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 使用例\n",
    "e = nn.Embedding(vocab_size, embedding_size) # 埋め込み層を定義\n",
    "\n",
    "# 単語のIDからなるベクトルを入力\n",
    "words_1D = torch.tensor([1,2,3], dtype=torch.long) # dtypeは必ずtorch.longにする\n",
    "print(\"Before embedding:\", words_1D.shape)\n",
    "embed_words = e(words_1D) # 単語IDをベクトルに変換\n",
    "print(\"After embedding:\", embed_words.shape)\n",
    "print()\n",
    "\n",
    "# 埋め込み層への入力は2Dでも可\n",
    "words_2D = torch.tensor([[1,2,3], [4,5,6]], dtype=torch.long)\n",
    "print(\"Before embedding:\", words_2D.shape)\n",
    "embed_words = e(words_2D)\n",
    "print(\"After embedding:\", embed_words.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. CBOWモデル定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目的関数はNegative Log Likelihood (NLL)です。\n",
    "\n",
    "単語$w_t$の条件付き確率$p(w_t|w_{t-c}, ... , w_{t-1}, w_{t+1}, ..., w_{t+c})$は、周囲の単語が$w_{t-c}, ... , w_{t-1}, w_{t+1}, ..., w_{t+c}$であるときにt番目の単語が$w_t$である確率を表します。周囲の単語ベクトルの和をとってから線形変換して次元を`embedding_size`から`vocab_size`に変換した後、softmax関数を介して得られた確率分布により求められます。\n",
    "\n",
    "\\begin{align}\n",
    "& {\\mathcal L}_{CBOW}=-\\frac{1}{T}\\sum_{t=1}^T \\log p(w_{t}|w_{t-c}, ... , w_{t-1}, w_{t+1}, ..., w_{t+c})\\\\\n",
    "& =-\\frac{1}{T}\\sum_{t=1}^T \\log ({\\rm softmax}((\\sum_{-c \\leq j \\leq c,j\\neq0}\\boldsymbol{v}_{t+j})^T\\boldsymbol{W})_{s_t})\n",
    "\\end{align}\n",
    "\n",
    "- $T$: バッチサイズ\n",
    "- $w_t$: 予測すべき単語\n",
    "- $c$: window幅(≧1)\n",
    "- $\\boldsymbol{v}_t$: $w_t$のembedding vector\n",
    "- $s_t$: $w_t$の単語ID\n",
    "- $\\boldsymbol{W}$: (embedding_size, vocab_size)の行列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        \"\"\"\n",
    "        :param vocab_size: int, 語彙の総数\n",
    "        :param embedding_size: int, 単語埋め込みベクトルの次元\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        # 埋め込み層\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        # 全結合層(バイアスなし)\n",
    "        self.linear = nn.Linear(self.embedding_size, self.vocab_size, bias=False)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "\n",
    "    def forward(self, batch_X, batch_Y):\n",
    "        \"\"\"\n",
    "        :param batch_X: torch.Tensor(dtype=torch.long), (batch_size, window*2)\n",
    "        :param batch_Y: torch.Tensor(dtype=torch.long), (batch_size,)\n",
    "        :return loss: torch.Tensor(dtype=torch.float), CBOWのloss\n",
    "        \"\"\"\n",
    "        emb_X = self.embedding(batch_X) # (batch_size, window*2, embedding_size)\n",
    "        # paddingした部分を無視するためにマスクをかけます\n",
    "        emb_X = emb_X * (batch_X != PAD).float().unsqueeze(-1) # (batch_size, window*2, embedding_size)\n",
    "        sum_X = torch.sum(emb_X, dim=1) # (batch_size, embedding_size)\n",
    "        lin_X = self.linear(sum_X) # (batch_size, vocab_size)\n",
    "        log_prob_X = F.log_softmax(lin_X, dim=-1) # (batch_size, vocab_size)\n",
    "        loss = F.nll_loss(log_prob_X, batch_Y)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. 訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル\n",
    "cbow = CBOW(vocab_size, embedding_size).to(device) # iLectで実行する場合warning (GPU is too old) が出ますが, 実行に問題はないので気にせず進めてください.\n",
    "# optimizer\n",
    "optimizer_cbow = optim.Adam(cbow.parameters())\n",
    "# データローダー\n",
    "dataloader_cbow = DataLoaderCBOW(id_text, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, inputs, optimizer=None, is_train=True):\n",
    "    \"\"\"lossを計算するための関数\n",
    "    \n",
    "    is_train=Trueならモデルをtrainモードに、\n",
    "    is_train=Falseならモデルをevaluationモードに設定します\n",
    "    \n",
    "    :param model: 学習させるモデル\n",
    "    :param inputs: モデルへの入力\n",
    "    :param optimizer: optimizer\n",
    "    :param is_train: bool, モデルtrainさせるか否か\n",
    "    \"\"\"\n",
    "    model.train(is_train)\n",
    "\n",
    "    # lossを計算します。\n",
    "    loss = model(*inputs)\n",
    "\n",
    "    if is_train:\n",
    "        # .backward()を実行する前にmodelのparameterのgradientを全て0にセットします\n",
    "        optimizer.zero_grad()\n",
    "        # parameterのgradientを計算します。\n",
    "        loss.backward()\n",
    "        # parameterのgradientを用いてparameterを更新します。\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_at = time.time()\n",
    "\n",
    "for batch_id, (batch_X, batch_Y) in enumerate(dataloader_cbow):\n",
    "    loss = compute_loss(cbow, (batch_X, batch_Y), optimizer=optimizer_cbow, is_train=True)\n",
    "    if batch_id % 100 == 0:\n",
    "        print(\"batch:{}, loss:{:.4f}\".format(batch_id, loss))\n",
    "    if batch_id >= n_batches:\n",
    "        break\n",
    "\n",
    "end_at = time.time()\n",
    "\n",
    "print(\"Elapsed time: {:.2f} [sec]\".format(end_at - start_at))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. 訓練済みパラメータの保存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CBOWの学習が終わった時の埋め込み層のパラメータが、各単語の特徴を表現していると捉えます。\n",
    "\n",
    "そのためCBOWの埋め込み層のパラメータのみを保存します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 埋め込み層のパラメータのみを保存する\n",
    "torch.save(cbow.embedding.weight.data.cpu().numpy(),  \"../data/cbow_embedding_{}b.pth\".format(n_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 保存したパラメータの読み込み方\n",
    "e = torch.load(\"../data/cbow_embedding_{}b.pth\".format(n_batches))\n",
    "e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Skipgram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ターゲットの単語($w(t)$)から周囲の単語($w(t-2)...w(t+2)$)を予測するタスクを解くことで単語のベクトルを学習します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../image/skipgram.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "出典:[Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. データローダーの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderSG(object):\n",
    "    \"\"\"Skipgramのためのデータローダー\"\"\"\n",
    "    def __init__(self, text, batch_size, window=3):\n",
    "        \"\"\"\n",
    "        :param text: list of list of int, 単語をIDに変換したデータセット\n",
    "        :param batch_size: int, ミニバッチのサイズ\n",
    "        :param window: int, 周辺単語と入力単語の最大距離\n",
    "        \"\"\"\n",
    "        self.text = text\n",
    "        self.batch_size = batch_size\n",
    "        self.window = window\n",
    "        self.s_pointer = 0 # データセット上を走査する文単位のポインタ\n",
    "        self.w_pointer = 0 # データセット上を走査する単語単位のポインタ\n",
    "        self.max_s_pointer = len(text) # データセットに含まれる文の総数\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        batch_X = []\n",
    "        batch_Y = []\n",
    "\n",
    "        while len(batch_X) < self.batch_size:\n",
    "            sen = self.text[self.s_pointer]\n",
    "            \n",
    "            # Skipgramでは入力が1単語\n",
    "            word_X = # WRITE ME\n",
    "\n",
    "            # 出力は周辺単語\n",
    "            start = # WRITE ME\n",
    "            word_Y = # WRITE ME\n",
    "            word_Y = # WRITE ME, paddingが必要\n",
    "\n",
    "            batch_X.append(word_X)\n",
    "            batch_Y.append(word_Y)\n",
    "            self.w_pointer += 1\n",
    "\n",
    "            if self.w_pointer >= len(sen):\n",
    "                self.w_pointer = 0\n",
    "                self.s_pointer += 1\n",
    "                if self.s_pointer >= self.max_s_pointer:\n",
    "                    self.s_pointer = 0\n",
    "                    raise StopIteration\n",
    "\n",
    "        batch_X = torch.tensor(batch_X, dtype=torch.long, device=device)\n",
    "        batch_Y = torch.tensor(batch_Y, dtype=torch.long, device=device)\n",
    "\n",
    "        return batch_X, batch_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips: torch.gather(input, dim, index)\n",
    "指定したdimに沿って、以下のような操作を行う。(inputとindexが3D tensorだった場合)\n",
    "\\begin{align}\n",
    "& out[ i ][ j ][ k ] = input[index[i][j][k]][j][k],~ if dim == 0\\\\\n",
    "& out[ i ][ j ][ k ] = input[i][index[i][j][k]][k],~ if dim == 1\\\\\n",
    "& out[ i ][ j ][ k ] = input[i][j][index[i][j][k]],~ if dim == 2\\\\\n",
    "\\end{align}\n",
    "\n",
    "詳しくはchap1を参照してください。\n",
    "\n",
    "※参考:https://pytorch.org/docs/stable/torch.html#torch.gather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. モデルの定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目的関数はNegative Log Likelihood (NLL)です。\n",
    "\n",
    "$p(w_{t+j}|w_t)$は、単語$w_t$の周囲に単語$w_{t+j}$が存在する確率を表します。この確率は単語$w_t$のembedding vectorを線形変換により次元を`embedding_size`から`vocab_size`に変換した後、softmax関数を介して得られた確率分布によって求められます。（下の数式の3行目から4行目に相当）\n",
    "\n",
    "実装する際は、各jについて同じ計算$\\log({\\rm softmax}(\\boldsymbol{v}_{t}^T\\boldsymbol{W})_{s_{t+j}})$をするのは非効率なので、$\\log({\\rm softmax}(\\boldsymbol{v}_{t}^T\\boldsymbol{W}))$を計算した後にすべてのjに対応する値をtorch.gatherを使って抽出できると計算時間を短縮できます。\n",
    "\n",
    "\\begin{align}\n",
    "& {\\mathcal L}_{SG}=-\\frac{1}{T}\\sum_{t=1}^T \\log p(w_{t-c}, ... , w_{t-1}, w_{t+1}, ..., w_{t+c}|w_{t})\\\\\n",
    "& = -\\frac{1}{T}\\sum_{t=1}^T \\log \\prod_{-c \\leq j \\leq c,j\\neq0} p(w_{t+j}|w_t)\\\\\n",
    "& =-\\frac{1}{T}\\sum_{t=1}^T \\sum_{-c \\leq j \\leq c,j\\neq0} \\log p(w_{t+j}|w_t)\\\\\n",
    "& =-\\frac{1}{T}\\sum_{t=1}^T \\sum_{-c \\leq j \\leq c,j\\neq0} \\log ({\\rm softmax}(\\boldsymbol{v}_{t}^T\\boldsymbol{W})_{s_{t+j}})\n",
    "\\end{align}\n",
    "\n",
    "- $T$: バッチサイズ\n",
    "- $w_t$: 入力単語\n",
    "- $c$: window幅(≧1)\n",
    "- $\\boldsymbol{v}_t$: $w_t$のembedding vector\n",
    "- $s_t$: $w_t$の単語ID\n",
    "- $\\boldsymbol{W}$: (embedding_size, vocab_size)の行列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skipgram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        \"\"\"\n",
    "        :param vocab_size: int, 語彙の総数\n",
    "        :param embedding_size: int, 単語埋め込みベクトルの次元\n",
    "        \"\"\"\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        self.linear = nn.Linear(self.embedding_size, self.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, batch_X, batch_Y):\n",
    "        \"\"\"\n",
    "        :param batch_X: torch.Tensor(dtype=torch.long), (batch_size,)\n",
    "        :param batch_Y: torch.Tensor(dtype=torch.long), (batch_size, window*2)\n",
    "        :return loss: torch.Tensor(dtype=torch.float), Skipgramのloss\n",
    "        \"\"\"\n",
    "        emb_X = # WRITE ME (batch_size, embedding_size)\n",
    "        lin_X = # WRITE ME (batch_size, vocab_size)\n",
    "        log_prob_X = # WRITE ME  (batch_size, vocab_size)、各単語の確率\n",
    "        log_prob_X = torch.gather(log_prob_X, 1, batch_Y) # (batch_size, window*2)\n",
    "        # paddingした単語のlossは計算しないようにマスクをかけます(=lossの該当部分を0にします)\n",
    "        log_prob_X = log_prob_X * (batch_Y != PAD).float() # (batch_size, window*2)\n",
    "        loss = log_prob_X.sum(1).mean().neg()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. 訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg = Skipgram(vocab_size, embedding_size).to(device)\n",
    "optimizer_sg = optim.Adam(sg.parameters())\n",
    "dataloader_sg = DataLoaderSG(id_text, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_at = time.time()\n",
    "for batch_id, (batch_X, batch_Y) in enumerate(dataloader_sg):\n",
    "    loss = compute_loss(sg, (batch_X, batch_Y), optimizer=optimizer_sg, is_train=True)\n",
    "    if batch_id % 100 == 0:\n",
    "        print(\"batch:{}, loss:{:.4f}\".format(batch_id, loss))\n",
    "    if batch_id >= n_batches:\n",
    "        break\n",
    "end_at = time.time()\n",
    "print(\"Elapsed time: {:.2f} [sec]\".format(end_at - start_at))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. 訓練済みパラメータの保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 埋め込み層のパラメータのみを保存する\n",
    "torch.save(sg.embedding.weight.data.cpu().numpy(), \"../data/sg_embedding_{}b.pth\".format(n_batches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Skipgram with Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips: Negative Sampling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Mikolov et al. 2013]によって提案された単語ベクトルの学習するために用いる方法の1つで、gensimのword2vecにも実装されています。\n",
    "Negative Samplingとは実際には存在しないデータ(=負例)をランダムにサンプリングすることです。\n",
    "\n",
    "提案された背景として、従来のSkipgramやCBOWでは文中におけるターゲットの単語($w_t$)とその周辺単語($w_{t+j}$)が共起する事象の尤度を最大化して単語ベクトルを学習させていました。ところがこの尤度ではsoftmax関数を使うため計算量が語彙数$n$(通常$10^5$から$10^7$ほど)に比例するので$O(n)$であり膨大でした。\n",
    "\n",
    "そこでsoftmax関数の代わりにsigmoid関数を使うことによって計算量を$O(1)$に抑える代わりに、単語ベクトルの質を担保するために新たにNegative Samplingを導入しました。\n",
    "元の論文では、実際に共起する単語ペア、すなわち正例の尤度を最大化し、同時にNegative Samplingによって得られた負例の尤度を最小化しています。\n",
    "\n",
    "参考:[Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0. Negative Sampling準備\n",
    "元論文では、負例をサンプリングする際に使う確率分布$P_n(w)$をコーパスにおける単語$w$の出現回数$U(w)$から以下のように定義しています。\n",
    "\n",
    "\\begin{align}\n",
    "P_n(w) = \\frac{U(w)^{3/4}}{\\sum_{w \\in W}U(w)^{3/4}}\n",
    "\\end{align}\n",
    "\n",
    "この確率分布の求め方はヒューリスティックですが元論文では試行錯誤の結果これが良かったと報告されています。恐らく3/4には高頻度語(is,theなど)ばかりがサンプリングされるのを防ぐ効果があったためだと思われます。\n",
    "\n",
    "実際に実装する際は、特殊なトークンであるPADやUNKはコーパスに登場しないことに注意してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative samplingに使う確率分布\n",
    "weights = np.power([0, 0] + list(vocab.raw_vocab.values()), 0.75)\n",
    "weights = weights / weights.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. データローダー\n",
    "3.Skipgramとは異なりデータローダーの中でNegative Samplingを行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderSGNS(object):\n",
    "    def __init__(self, text, batch_size, window=3, n_negative=5, weights=None):\n",
    "        \"\"\"\n",
    "        :param text: list of list of int, 単語をIDに変換したデータセット\n",
    "        :param batch_size: int, ミニバッチのサイズ\n",
    "        :param window: int, 周辺単語と入力単語の最大距離\n",
    "        :param n_negative: int, 負例の数\n",
    "        :param weights: numpy.ndarray, Negative Samplingで使う確率分布\n",
    "        \"\"\"\n",
    "        self.text = text\n",
    "        self.batch_size = batch_size\n",
    "        self.window = window\n",
    "        self.n_negative = n_negative\n",
    "        self.weights = None\n",
    "        if weights is not None:\n",
    "            self.weights = torch.FloatTensor(weights) # negative samplingに使う確率分布\n",
    "        self.s_pointer = 0 # 文のポインタ\n",
    "        self.w_pointer = 0 # 単語のポインタ\n",
    "        self.max_s_pointer = len(text)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        batch_X = []\n",
    "        batch_Y = []\n",
    "        batch_N = [] # 負例\n",
    "        while len(batch_X) < self.batch_size:\n",
    "            sen = self.text[self.s_pointer]\n",
    "            start = max(0, self.w_pointer - self.window)\n",
    "            word_X = sen[self.w_pointer]\n",
    "            word_Y = sen[start:self.w_pointer] + \\\n",
    "                sen[self.w_pointer + 1:self.w_pointer + self.window + 1]\n",
    "            word_Y = pad_seq(word_Y, self.window * 2)\n",
    "            batch_X.append(word_X)\n",
    "            batch_Y.append(word_Y)\n",
    "\n",
    "            # 多項分布で負例をサンプリング\n",
    "            # 実装を簡略化するために、正例の除去は行っていません\n",
    "            negative_samples = torch.multinomial(self.weights, self.n_negative) # (n_negative,)\n",
    "            batch_N.append(negative_samples.unsqueeze(0)) # (1, n_negative)\n",
    "\n",
    "            self.w_pointer += 1\n",
    "            if self.w_pointer >= len(sen):\n",
    "                self.w_pointer = 0\n",
    "                self.s_pointer += 1\n",
    "                if self.s_pointer >= self.max_s_pointer:\n",
    "                    self.s_pointer = 0\n",
    "                    raise StopIteration\n",
    "\n",
    "        batch_X = torch.tensor(batch_X, dtype=torch.long, device=device)\n",
    "        batch_Y = torch.tensor(batch_Y, dtype=torch.long, device=device)\n",
    "        batch_N = torch.cat(batch_N, dim=0).to(device) # (batch_size, n_negative)\n",
    "\n",
    "        return batch_X, batch_Y, batch_N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. モデルの定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skipgram with Negative Sampling (SGNS)の目的関数では、$p(w_{t+j}|w_t)=\\sigma (\\boldsymbol{v}'^T_{t+j}\\boldsymbol{v}_t)$と定義することで計算量を削減しています。活性化関数にsoftmax関数ではなくsigmoid関数を用いているためです。\n",
    "\n",
    "正例の尤度の最大化に加えてnegative samplingをして負例の尤度を最小化するため目的関数は以下のように記述されます。\n",
    "\n",
    "\\begin{align}\n",
    "& {\\mathcal L}_{SGNS}= \\frac{1}{T} \\sum_{t=1}^T NLL(w_t) \\\\\n",
    "& =-\\frac{1}{T} \\sum_{t=1}^T \\log \\left( \\prod_{-c \\leq j \\leq c,j\\neq0} p(w_{t+j}|w_t) \\prod_{w_n \\in S} \\left( 1 - p(w_n|w_t) \\right)  \\right) \\\\\n",
    "& =-\\frac{1}{T} \\sum_{t=1}^T \\left( \\sum_{-c \\leq j \\leq c,j\\neq0} \\log p(w_{t+j}|w_t) + \\sum_{w_n \\in S} \\log (1 - p(w_n|w_t)) \\right) \\\\\n",
    "& =-\\frac{1}{T} \\sum_{t=1}^T \\left( \\sum_{-c \\leq j \\leq c,j\\neq0} \\log \\sigma (\\boldsymbol{v}'^T_{t+j} \\boldsymbol{v}_t) + \\sum_{w_n \\in S} \\log (1 - \\sigma (\\boldsymbol{v}'^T_n\\boldsymbol{v}_t)) \\right) \\\\\n",
    "& =-\\frac{1}{T} \\sum_{t=1}^T \\left( \\sum_{-c \\leq j \\leq c,j\\neq0} \\log \\sigma (\\boldsymbol{v}'^T_{t+j}\\boldsymbol{v}_t) + \\sum_{w_n \\in S} \\log \\sigma (-\\boldsymbol{v}'^T_n\\boldsymbol{v}_t) \\right) \\\\\n",
    "\\end{align}\n",
    "\n",
    "- $T$: バッチサイズ\n",
    "- $w_t$: 入力単語\n",
    "- $c$: window幅(≧1)\n",
    "- $S$: ランダムに選んだNegative Sampleの集合\n",
    "- $\\boldsymbol{v}_t$: 入力単語$w_t$のembedding vector\n",
    "- $\\boldsymbol{v}'_t$: 出力単語$w_t$のembedding vector\n",
    "- $\\sigma$: シグモイド関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGNS(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        \"\"\"\n",
    "        :param vocab_size: int, 語彙の総数\n",
    "        :param embedding_size: int, 単語埋め込みベクトルの次元\n",
    "        \"\"\"\n",
    "        super(SGNS, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        # 入力単語の埋め込み層\n",
    "        self.i_embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        # 出力単語の埋め込み層\n",
    "        self.o_embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.i_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.o_embedding.weight)\n",
    "\n",
    "    def forward(self, batch_X, batch_Y, batch_N):\n",
    "        \"\"\"\n",
    "        :param batch_x: torch.Tensor(dtype=torch.long), (batch_size,)\n",
    "        :param batch_y: torch.Tensor(dtype=torch.long), (batch_size, window*2)\n",
    "        :param batch_n: torch.Tensor(dtype=torch.long), (batch_size, n_negative)\n",
    "        \"\"\"\n",
    "        embed_X = self.i_embedding(batch_X).unsqueeze(2) # (batch_size, embedding_size, 1)\n",
    "        embed_Y = self.o_embedding(batch_Y) # (batch_size, window*2, embedding_size)\n",
    "        embed_N = self.o_embedding(batch_N).neg() # (batch_size, n_negative, embedding_size)\n",
    "        loss_Y = torch.bmm(embed_Y, embed_X).squeeze().sigmoid().log() # (batch_size, window*2)\n",
    "        loss_Y = loss_Y * (batch_Y != PAD).float() # (batch_size, window*2)\n",
    "        loss_Y = loss_Y.sum(1) # (batch_size,)\n",
    "        loss_N = torch.bmm(embed_N, embed_X).squeeze().sigmoid().log().sum(1) # (batch_size,)\n",
    "        return -(loss_Y + loss_N).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. 訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sgns = SGNS(vocab_size, embedding_size).to(device)\n",
    "optimizer_sgns = optim.Adam(sgns.parameters())\n",
    "dataloader_sgns = DataLoaderSGNS(id_text, batch_size, n_negative=5, weights=weights)\n",
    "start_at = time.time()\n",
    "for batch_id, (batch_X, batch_Y, batch_N) in enumerate(dataloader_sgns):\n",
    "    loss = compute_loss(sgns, (batch_X, batch_Y, batch_N), optimizer=optimizer_sgns, is_train=True)\n",
    "    if batch_id % 100 == 0:\n",
    "        print(\"batch:{}, loss:{:.4f}\".format(batch_id, loss))\n",
    "    if batch_id >= n_batches:\n",
    "        break\n",
    "end_at = time.time()\n",
    "print(\"Elapsed time: {:.2f} [sec]\".format(end_at - start_at))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. 保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddingのパラメータのみを保存する\n",
    "torch.save(sgns.i_embedding.weight.data.cpu().numpy(), \"../data/sgns_embedding_{}b.pth\".format(n_batches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Word Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単語の意味が似ていれば単語ベクトルも似たようなものになっているはずです。\n",
    "\n",
    "単語同士が\"似ている\"かどうかを判断するために、しばしば単語ベクトル同士のcos類似度が用いられます。\n",
    "\n",
    "cos類似度は単語ベクトル同士のなす角$\\theta$を用いて、\n",
    "\n",
    "\\begin{align}\n",
    " cos\\theta=\\frac{\\boldsymbol{u}^T\\boldsymbol{v}}{|\\boldsymbol{u}||\\boldsymbol{v}|}\n",
    "\\end{align}\n",
    "\n",
    "のように定義されます。単語同士の意味が似ているほどcos類似度は1に近くなると考えられています。\n",
    "\n",
    "Word2Vecで学習すると、どの単語とどの単語が似ていると判断されるのか見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_word_similarity(embedding_path, word, n):\n",
    "    \"\"\"\n",
    "    与えられた単語に最も似ている単語とcos類似度を返す関数\n",
    "\n",
    "    :param embedding_path: str, 保存した埋め込み層のパラメータのパス\n",
    "    :param word: str, 単語\n",
    "    :param n: int\n",
    "    :return out: str, 上位n個の類似単語とそのcos類似度\n",
    "    \"\"\"\n",
    "    embedding = torch.load(embedding_path)\n",
    "\n",
    "    # 単語ベクトルを全て単位ベクトルにする\n",
    "    norm = np.linalg.norm(embedding, ord=2, axis=1, keepdims=True)\n",
    "    norm = np.where(norm==0, 1, norm) # 0で割ることを避ける\n",
    "    embedding /= norm\n",
    "    e = embedding[vocab.word2id[word]]\n",
    "\n",
    "    # 単語ベクトル同士のcos類似度を計算する\n",
    "    cos_sim = np.dot(embedding, e.reshape(-1, 1)).reshape(-1,)\n",
    "    most_sim = np.argsort(cos_sim)[::-1][1:n+1] # 自分は除く\n",
    "    most_sim_words = [vocab.id2word[_id] for _id in most_sim]\n",
    "    top_cos_sim = cos_sim[most_sim]\n",
    "    out = \", \".join([w+\"({:.4f})\".format(v) for w, v in zip(most_sim_words, top_cos_sim)])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 300バッチだけ学習した時\n",
    "models = [\"cbow\", \"sg\", \"sgns\"]\n",
    "for model in models:\n",
    "    print(model+\"\\t:\", compute_word_similarity(\n",
    "        \"../data/\" + model + \"_embedding_300b.pth\", \"私\", 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 600バッチだけ学習した時\n",
    "models = [\"cbow\", \"sg\", \"sgns\"]\n",
    "for model in models:\n",
    "    print(model+\"\\t:\", compute_word_similarity(\n",
    "        \"../data/\" + model + \"_embedding_600b.pth\", \"私\", 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 900バッチだけ学習した時\n",
    "models = [\"cbow\", \"sg\", \"sgns\"]\n",
    "for model in models:\n",
    "    print(model+\"\\t:\", compute_word_similarity(\n",
    "        \"../data/\" + model + \"_embedding_900b.pth\", \"私\", 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Word Analogy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次は単語ベクトルの足し算・引き算によって単語を類推できるかどうかによってモデルを評価します。\n",
    "\n",
    "ここでは$v($\"父\"$) - v($\"男\"$) + v($\"女\"$)$とcos類似度で測ったときに最も似ている単語を列挙します。\n",
    "\n",
    "より\"母\"が上位に来ているモデルが優れていると判断できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_word_analogy(embedding_path, word1, word2, word3, n):\n",
    "    \"\"\"word1 - word2 + word3と最も類似度の高い単語を返す関数\n",
    "\n",
    "    :param embedding_path: str, 保存した埋め込み層のパラメータのパス\n",
    "    :param word1: str, 単語\n",
    "    :param word2: str, 単語\n",
    "    :param word3: str, 単語\n",
    "    :param n: int\n",
    "    :return out: str, 上位n個の類似単語とそのcos類似度\n",
    "    \"\"\"\n",
    "    embedding = torch.load(embedding_path)\n",
    "    w1 = vocab.word2id[word1]\n",
    "    w2 = vocab.word2id[word2]\n",
    "    w3 = vocab.word2id[word3]\n",
    "    v1 = embedding[w1]\n",
    "    v2 = embedding[w2]\n",
    "    v3 = embedding[w3]\n",
    "    v = v1 - v2 + v3\n",
    "    v /= np.linalg.norm(v, ord=2)\n",
    "    \n",
    "    norm = np.linalg.norm(embedding, ord=2, axis=1, keepdims=True)\n",
    "    norm = np.where(norm==0, 1, norm) # 0で割ることを避ける\n",
    "    embedding /= norm\n",
    "    \n",
    "    cos_sim = np.dot(embedding, v.reshape(-1, 1)).reshape(-1,)\n",
    "    most_sim = np.argsort(cos_sim)[::-1][0:n]\n",
    "    most_sim_words = [vocab.id2word[_id] for _id in most_sim]\n",
    "    top_cos_sim = cos_sim[most_sim]\n",
    "    out = \", \".join([w+\"({:.4f})\".format(v) for w, v in zip(most_sim_words, top_cos_sim)])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 300バッチだけ学習した時\n",
    "models = [\"cbow\", \"sg\", \"sgns\"]\n",
    "for model in models:\n",
    "    print(model+\"\\t:\", compute_word_analogy(\n",
    "        \"../data/\" + model + \"_embedding_300b.pth\", \"父\", \"男\", \"女\", 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 600バッチだけ学習した時\n",
    "models = [\"cbow\", \"sg\", \"sgns\"]\n",
    "for model in models:\n",
    "    print(model+\"\\t:\", compute_word_analogy(\n",
    "        \"../data/\" + model + \"_embedding_600b.pth\", \"父\", \"男\", \"女\", 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 900バッチだけ学習した時\n",
    "models = [\"cbow\", \"sg\", \"sgns\"]\n",
    "for model in models:\n",
    "    print(model+\"\\t:\", compute_word_analogy(\n",
    "        \"../data/\" + model + \"_embedding_900b.pth\", \"父\", \"男\", \"女\", 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考文献"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 元論文\n",
    "    * [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)\n",
    "    * [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)\n",
    "* [スタンフォード大学のDeep Learning for NLP講座の講義ノート](https://cs224d.stanford.edu/lecture_notes/notes1.pdf)\n",
    "* [gensimでのWord2Vec実装](https://radimrehurek.com/gensim/models/word2vec.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
