{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第4回 演習（2）Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TransformerはRNNやCNNを使用せず、Attentionのみを用いるSeq2Seqモデルです。\n",
    "\n",
    "並列計算が可能なためRNNに比べて計算が高速な上、Self-Attentionと呼ばれる機構を用いることにより、局所的な位置しか参照できないCNNと異なり、系列内の任意の位置の情報を参照することを可能にしています。\n",
    "\n",
    "その他にもいくつかの工夫が加えられており、翻訳に限らない自然言語処理のあらゆるタスクで圧倒的な性能を示すことが知られています。\n",
    "\n",
    "原論文：[Attention is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "参考実装：https://github.com/jadore801120/attention-is-all-you-need-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk import bleu_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "try:\n",
    "    from utils import Vocab\n",
    "except ImportError:  # iLect環境\n",
    "    import os\n",
    "    os.chdir('/root/userspace/chap4/materials')\n",
    "    from utils import Vocab\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.manual_seed(1)\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "UNK = 1\n",
    "BOS = 2\n",
    "EOS = 3\n",
    "\n",
    "PAD_TOKEN = '<PAD>'\n",
    "UNK_TOKEN = '<UNK>'\n",
    "BOS_TOKEN = '<S>'\n",
    "EOS_TOKEN = '</S>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. データセット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    テキストファイルからデータを読み込む\n",
    "    :param file_path: str, テキストファイルのパス\n",
    "    :return data: list, 文章（単語のリスト）のリスト\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for line in open(file_path, encoding='utf-8'):\n",
    "        words = line.strip().split()  # スペースで単語を分割\n",
    "        data.append(words)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = load_data('../data/train.en')\n",
    "train_Y = load_data('../data/train.ja')\n",
    "# 演習用にデータサイズを縮小\n",
    "train_X = train_X[:len(train_X)//2]\n",
    "train_Y = train_Y[:len(train_Y)//2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 訓練データと検証データに分割\n",
    "train_X, valid_X, train_Y, valid_Y = train_test_split(train_X, train_Y, test_size=0.2, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN_COUNT = 2  # 語彙に含める単語の最低出現回数\n",
    "\n",
    "word2id = {\n",
    "    PAD_TOKEN: PAD,\n",
    "    BOS_TOKEN: BOS,\n",
    "    EOS_TOKEN: EOS,\n",
    "    UNK_TOKEN: UNK,\n",
    "    }\n",
    "\n",
    "vocab_X = Vocab(word2id=word2id)\n",
    "vocab_Y = Vocab(word2id=word2id)\n",
    "vocab_X.build_vocab(train_X, min_count=MIN_COUNT)\n",
    "vocab_Y.build_vocab(train_Y, min_count=MIN_COUNT)\n",
    "\n",
    "vocab_size_X = len(vocab_X.id2word)\n",
    "vocab_size_Y = len(vocab_Y.id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_to_ids(vocab, sentence):\n",
    "    \"\"\"\n",
    "    単語のリストをインデックスのリストに変換する\n",
    "    :param vocab: Vocabのインスタンス\n",
    "    :param sentence: list of str\n",
    "    :return indices: list of int\n",
    "    \"\"\"\n",
    "    ids = [vocab.word2id.get(word, UNK) for word in sentence]\n",
    "    ids = [BOS] + ids + [EOS]  # </S>トークンを末尾に加える\n",
    "#     ids += [EOS]  # EOSを末尾に加える\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = [sentence_to_ids(vocab_X, sentence) for sentence in train_X]\n",
    "train_Y = [sentence_to_ids(vocab_Y, sentence) for sentence in train_Y]\n",
    "valid_X = [sentence_to_ids(vocab_X, sentence) for sentence in valid_X]\n",
    "valid_Y = [sentence_to_ids(vocab_Y, sentence) for sentence in valid_Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataLoader(object):\n",
    "    def __init__(self, src_insts, tgt_insts, batch_size, shuffle=True):\n",
    "        \"\"\"\n",
    "        :param src_insts: list, 入力言語の文章（単語IDのリスト）のリスト\n",
    "        :param tgt_insts: list, 出力言語の文章（単語IDのリスト）のリスト\n",
    "        :param batch_size: int, バッチサイズ\n",
    "        :param shuffle: bool, サンプルの順番をシャッフルするか否か\n",
    "        \"\"\"\n",
    "        self.data = list(zip(src_insts, tgt_insts))\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.start_index = 0\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        if self.shuffle:\n",
    "            self.data = shuffle(self.data, random_state=random_state)\n",
    "        self.start_index = 0\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "\n",
    "        def preprocess_seqs(seqs):\n",
    "            # パディング\n",
    "            max_length = max([len(s) for s in seqs])\n",
    "            data = [s + [PAD] * (max_length - len(s)) for s in seqs]\n",
    "            # 単語の位置を表現するベクトルを作成\n",
    "            positions = [[pos+1 if w != PAD else 0 for pos, w in enumerate(seq)] for seq in data]\n",
    "            # テンソルに変換\n",
    "            data_tensor = torch.tensor(data, dtype=torch.long, device=device)\n",
    "            position_tensor = torch.tensor(positions, dtype=torch.long, device=device)\n",
    "            return data_tensor, position_tensor            \n",
    "\n",
    "        # ポインタが最後まで到達したら初期化する\n",
    "        if self.start_index >= len(self.data):\n",
    "            self.reset()\n",
    "            raise StopIteration()\n",
    "\n",
    "        # バッチを取得して前処理\n",
    "        src_seqs, tgt_seqs = zip(*self.data[self.start_index:self.start_index+self.batch_size])\n",
    "        src_data, src_pos = preprocess_seqs(src_seqs)\n",
    "        tgt_data, tgt_pos = preprocess_seqs(tgt_seqs)\n",
    "\n",
    "        # ポインタを更新する\n",
    "        self.start_index += self.batch_size\n",
    "\n",
    "        return (src_data, src_pos), (tgt_data, tgt_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 各モジュールの定義\n",
    "Transformerのモデルも大きくはEncoder(下図左側）とDecoder（下図右側）からなっており、それぞれ灰色で表されているブロックをN個積み重ねた構造となっています。\n",
    "\n",
    "EncoderとDecoderは\n",
    "- Positional Encoding: 入出力の単語のEmbedding時に単語の位置情報を埋め込む\n",
    "- Scaled Dot-Product Attention: 内積でAttentionを計算し、スケーリングを行う\n",
    "- Multi-head Attention: Scaled Dot-Product Attentionを複数のヘッドで並列化する（下図橙色）\n",
    "- Position-Wise Feed Forward Network: 単語列の位置ごとに独立して処理を行う（下図水色）\n",
    "\n",
    "など、いくつかのモジュールから構成されているため、それぞれのモジュールを定義していきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/Transformer.png\" style=\"height: 600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ① Positional Encoding\n",
    "Transformerは系列の処理にRNNやCNNを使用しないので、そのままでは単語列の語順を考慮することができません。\n",
    "\n",
    "そのため、入力系列の埋込行列に単語の位置情報を埋め込むPositional Encodingを加算します。\n",
    "\n",
    "Positional Encodingの行列${\\bf PE}$の各成分は次式で表されます。\n",
    "\n",
    "$PE_{(pos, 2i)} = \\sin(pos/10000^{2i/d_{model}})$\n",
    "\n",
    "$PE_{(pos, 2i+1)} = \\cos(pos/10000^{2i/d_{model}})$\n",
    "\n",
    "ここで$pos$は単語の位置を、$i$は成分の次元を表しています。\n",
    "\n",
    "Positional Encodingの各次元は、波長が$2\\pi$から$10000*2\\pi$に幾何学的に伸びる正弦波に対応します。\n",
    "\n",
    "未知の系列長にも対応可能であり、また学習は不要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def position_encoding_init(n_position, d_pos_vec):\n",
    "    \"\"\"\n",
    "    Positional Encodingのための行列の初期化を行う\n",
    "    :param n_position: int, 系列長\n",
    "    :param d_pos_vec: int, 隠れ層の次元数\n",
    "    :return: torch.tensor, size=(n_position, d_pos_vec)\n",
    "    \"\"\"\n",
    "    # PADがある単語の位置はpos=0にしておき、position_encも0にする\n",
    "    position_enc = np.array([\n",
    "        [pos / np.power(10000, 2 * (j // 2) / d_pos_vec) for j in range(d_pos_vec)]\n",
    "        if pos != 0 else np.zeros(d_pos_vec) for pos in range(n_position)])\n",
    "\n",
    "    position_enc[1:, 0::2] = np.sin(position_enc[1:, 0::2]) # dim 2i\n",
    "    position_enc[1:, 1::2] = np.cos(position_enc[1:, 1::2]) # dim 2i+1\n",
    "    return torch.tensor(position_enc, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positional Encodingを可視化すると以下のようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = position_encoding_init(50, 256).numpy()\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.heatmap(pe, cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "縦軸が単語の位置を、横軸が成分の次元を表しており、濃淡が加算される値です。\n",
    "\n",
    "ここでは最大系列長を50、隠れ層の次元数を256としました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ② Multihead Attention\n",
    "\n",
    "\n",
    "#### Source-Target-AttentionとSelf-Attention\n",
    "\n",
    "Attentionは一般に、queryベクトルとkeyベクトルの類似度を求めて、その正規化した重みをvalueベクトルに適用して値を取り出す処理を行います。\n",
    "\n",
    "一般的な翻訳モデルで用いられるAttentionはSource-Target-Attentionと呼ばれ、この場合queryはDecoderの隠れ状態(Target)、keyはEncoderの隠れ状態（Source）、valueもEncoderの隠れ状態（Source）で表現されるのが一般的です。\n",
    "モデル全体の図では、右側のDecoderのブロックの中央にあるAttentionがこれに該当します。\n",
    "\n",
    "\n",
    "Transformerでは、このSource-Target-Attentionに加えて、query、key、valueを同じ系列内で定義するSelf-Attentionの機構を用います。これにより、ある単語位置の出力を求める際にあらゆる位置を参照できるため、局所的な位置しか参照できない畳み込み層よりも良い性能を発揮できると言われています。\n",
    "モデル全体の図では、左側のEncoderブロックと右側のDecoderのブロックの下部にあるAttentionがこれに該当します。\n",
    "\n",
    "<img src=\"../images/Attention.png\" style=\"width:80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformerでは、Scaled Dot-Product Attentionと呼ばれるAttentionを、複数のヘッドで並列に行うMulti-Head Attentionによって、Source-Target-AttentionとSelf-Attentionを表現します。\n",
    "\n",
    "以下では、Scaled Dot-Product AttentionとMulti-Head Attentionについて順に説明していきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaled Dot-Product Attention\n",
    "Attentionには、注意の重みを隠れ層 1 つのフィードフォワードネットワークで求めるAdditive Attentionと、注意の重みを内積で求めるDot-Product Attentionが存在します。  一般に、Dot-Product Attentionのほうがパラメータが少なく高速であり、Transformerでもこちらを使います。\n",
    "\n",
    "\n",
    "Tranformerではさらなる工夫として、query($Q$)とkey($K$)の内積をスケーリング因子 $\\sqrt{d_k}$ で除算します。\n",
    "\n",
    "$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$ \n",
    "\n",
    "\n",
    "これは、$d_k$（keyベクトルの次元数）が大きい場合に内積が大きくなりすぎて逆伝播のsoftmaxの勾配が極端に小さくなることを防ぐ役割を果たします。\n",
    "\n",
    "<img src=\"../images/Multihead.png\" style=\"height: 400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_model, attn_dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param d_model: int, 隠れ層の次元数\n",
    "        :param attn_dropout: float, ドロップアウト率\n",
    "        \"\"\"\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.temper = np.power(d_model, 0.5)  # スケーリング因子\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, q, k, v, attn_mask):\n",
    "        \"\"\"\n",
    "        :param q: torch.tensor, queryベクトル, \n",
    "            size=(n_head*batch_size, len_q, d_k)\n",
    "        :param k: torch.tensor, key, \n",
    "            size=(n_head*batch_size, len_k, d_k)\n",
    "        :param v: torch.tensor, valueベクトル, \n",
    "            size=(n_head*batch_size, len_v, d_v)\n",
    "        :param attn_mask: torch.tensor, Attentionに適用するマスク, \n",
    "            size=(n_head*batch_size, len_q, len_k)\n",
    "        :return output: 出力ベクトル, \n",
    "            size=(n_head*batch_size, len_q, d_v)\n",
    "        :return attn: Attention\n",
    "            size=(n_head*batch_size, len_q, len_k)\n",
    "        \"\"\"\n",
    "        # QとKの内積(上図右側`MatMul`)でAttentionの重みを求め、スケーリングする(上図右側`Scale`)\n",
    "        attn = # WRITE ME! Hint: torch.bmmを使う (n_head*batch_size, len_q, len_k)\n",
    "        # Attentionをかけたくない部分がある場合は、その部分を負の無限大に飛ばしてSoftmaxの値が0になるようにする(上図右側`Mask(opt.)`)\n",
    "        attn.data.masked_fill_(attn_mask, -float('inf'))\n",
    "        \n",
    "        attn = self.softmax(attn)  # (上図右側`SoftMax`)\n",
    "        attn = self.dropout(attn)\n",
    "        output = # WRITE ME! attnとVの内積を計算 (上図右側`MatMul`)\n",
    "\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Head Attention\n",
    "TransformerではAttentionを複数のヘッドで並列に行うMulti-Head Attentionを採用しています。\n",
    "\n",
    "複数のヘッドでAttentionを行うことにより、各ヘッドが異なる位置の異なる部分空間を処理でき、精度が向上するとされています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param n_head: int, ヘッド数\n",
    "        :param d_model: int, 隠れ層の次元数\n",
    "        :param d_k: int, keyベクトルの次元数\n",
    "        :param d_v: int, valueベクトルの次元数\n",
    "        :param dropout: float, ドロップアウト率\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "        # 各ヘッドごとに異なる重みで線形変換を行うための重み\n",
    "        # nn.Parameterを使うことで、Moduleのパラメータとして登録できる\n",
    "        self.w_qs = nn.Parameter(torch.empty([n_head, d_model, d_k], dtype=torch.float))\n",
    "        self.w_ks = nn.Parameter(torch.empty([n_head, d_model, d_k], dtype=torch.float))\n",
    "        self.w_vs = nn.Parameter(torch.empty([n_head, d_model, d_v], dtype=torch.float))\n",
    "        # nn.init.xavier_normal_で重みの値を初期化\n",
    "        nn.init.xavier_normal_(self.w_qs)\n",
    "        nn.init.xavier_normal_(self.w_ks)\n",
    "        nn.init.xavier_normal_(self.w_vs)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(d_model)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.proj = nn.Linear(n_head*d_v, d_model)  # 複数ヘッド分のAttentionの結果を元のサイズに写像するための線形層\n",
    "        # nn.init.xavier_normal_で重みの値を初期化\n",
    "        nn.init.xavier_normal_(self.proj.weight)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, q, k, v, attn_mask=None):\n",
    "        \"\"\"\n",
    "        :param q: torch.tensor, queryベクトル, \n",
    "            size=(batch_size, len_q, d_model)\n",
    "        :param k: torch.tensor, key, \n",
    "            size=(batch_size, len_k, d_model)\n",
    "        :param v: torch.tensor, valueベクトル, \n",
    "            size=(batch_size, len_v, d_model)\n",
    "        :param attn_mask: torch.tensor, Attentionに適用するマスク, \n",
    "            size=(batch_size, len_q, len_k)\n",
    "        :return outputs: 出力ベクトル, \n",
    "            size=(batch_size, len_q, d_model)\n",
    "        :return attns: Attention\n",
    "            size=(n_head*batch_size, len_q, len_k)\n",
    "            \n",
    "        \"\"\"\n",
    "        d_k, d_v = self.d_k, self.d_v\n",
    "        n_head = self.n_head\n",
    "\n",
    "        # residual connectionのための入力\n",
    "        residual = q\n",
    "\n",
    "        batch_size, len_q, d_model = q.size()\n",
    "        batch_size, len_k, d_model = k.size()\n",
    "        batch_size, len_v, d_model = v.size()\n",
    "\n",
    "        # 複数ヘッド化\n",
    "        # torch.repeat または .repeatで指定したdimに沿って同じテンソルを作成\n",
    "        q_s = q.repeat(n_head, 1, 1) # (n_head*batch_size, len_q, d_model)\n",
    "        k_s = k.repeat(n_head, 1, 1) # (n_head*batch_size, len_k, d_model)\n",
    "        v_s = v.repeat(n_head, 1, 1) # (n_head*batch_size, len_v, d_model)\n",
    "        # ヘッドごとに並列計算させるために、n_headをdim=0に、batch_sizeをdim=1に寄せる\n",
    "        q_s = q_s.view(n_head, -1, d_model) # (n_head, batch_size*len_q, d_model)\n",
    "        k_s = k_s.view(n_head, -1, d_model) # (n_head, batch_size*len_k, d_model)\n",
    "        v_s = v_s.view(n_head, -1, d_model) # (n_head, batch_size*len_v, d_model)\n",
    "\n",
    "        # 各ヘッドで線形変換を並列計算(上図左側`Linear`)\n",
    "        q_s = torch.bmm(q_s, self.w_qs)  # (n_head, batch_size*len_q, d_k)\n",
    "        k_s = torch.bmm(k_s, self.w_ks)  # (n_head, batch_size*len_k, d_k)\n",
    "        v_s = torch.bmm(v_s, self.w_vs)  # (n_head, batch_size*len_v, d_v)\n",
    "        # Attentionは各バッチ各ヘッドごとに計算させるためにbatch_sizeをdim=0に寄せる\n",
    "        q_s = q_s.view(-1, len_q, d_k)   # (n_head*batch_size, len_q, d_k)\n",
    "        k_s = k_s.view(-1, len_k, d_k)   # (n_head*batch_size, len_k, d_k)\n",
    "        v_s = v_s.view(-1, len_v, d_v)   # (n_head*batch_size, len_v, d_v)\n",
    "\n",
    "        # Attentionを計算(上図左側`Scaled Dot-Product Attention * h`)\n",
    "        outputs, attns = self.attention(q_s, k_s, v_s, attn_mask=attn_mask.repeat(n_head, 1, 1))\n",
    "        # outputs: (n_head*batch_size, len_q, d_v)\n",
    "        # attns: (n_head*batch_size, len_q, len_k)\n",
    "        \n",
    "        # 各ヘッドの結果を連結(上図左側`Concat`)\n",
    "        # torch.splitでbatch_sizeごとのn_head個のテンソルに分割\n",
    "        outputs = torch.split(outputs, batch_size, dim=0)  # (batch_size, len_q, d_v) * n_head\n",
    "        # dim=-1で連結\n",
    "        outputs = torch.cat(outputs, dim=-1)  # (batch_size, len_q, n_head*d_v)\n",
    "\n",
    "        # residual connectionのために元の大きさに写像(上図左側`Linear`)\n",
    "        outputs = self.proj(outputs)  # (batch_size, len_q, d_model)\n",
    "        outputs = self.dropout(outputs)\n",
    "        outputs = self.layer_norm(outputs + residual)\n",
    "\n",
    "        return outputs, attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ③ Position-Wise Feed Forward Network\n",
    "単語列の位置ごとに独立して処理する2層のネットワークであるPosition-Wise Feed Forward Networkを定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    :param d_hid: int, 隠れ層1層目の次元数\n",
    "    :param d_inner_hid: int, 隠れ層2層目の次元数\n",
    "    :param dropout: float, ドロップアウト率\n",
    "    \"\"\"\n",
    "    def __init__(self, d_hid, d_inner_hid, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Conv1d(d_hid, d_inner_hid, 1)\n",
    "        self.w_2 = nn.Conv1d(d_inner_hid, d_hid, 1)\n",
    "        self.layer_norm = nn.LayerNorm(d_hid)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: torch.tensor,\n",
    "            size=(batch_size, max_length, d_hid)\n",
    "        :return: torch.tensor,\n",
    "            size=(batch_size, max_length, d_hid) \n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        output = self.relu(self.w_1(x.transpose(1, 2)))\n",
    "        output = self.w_2(output).transpose(2, 1)\n",
    "        output = self.dropout(output)\n",
    "        return self.layer_norm(output + residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ④ Masking\n",
    "TransfomerではAttentionに対して2つのマスクを定義します。\n",
    "\n",
    "一つはkey側の系列のPADトークンに対してAttentionを行わないようにするマスクです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_attn_padding_mask(seq_q, seq_k):\n",
    "    \"\"\"\n",
    "    keyのPADに対するattentionを0にするためのマスクを作成する\n",
    "    :param seq_q: tensor, queryの系列, size=(batch_size, len_q)\n",
    "    :param seq_k: tensor, keyの系列, size=(batch_size, len_k)\n",
    "    :return pad_attn_mask: tensor, size=(batch_size, len_q, len_k)\n",
    "    \"\"\"\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    pad_attn_mask = seq_k.data.eq(PAD).unsqueeze(1)   # (N, 1, len_k)\n",
    "    pad_attn_mask = pad_attn_mask.expand(batch_size, len_q, len_k) # (N, len_q, len_k)\n",
    "    return pad_attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_seq_q = torch.tensor([[1, 2, 3]])\n",
    "_seq_k = torch.tensor([[4, 5, 6, 7, PAD]])\n",
    "_mask = get_attn_padding_mask(_seq_q, _seq_k)  # 行がquery、列がkeyに対応し、key側がPAD(=0)の時刻だけ1で他が0の行列ができる\n",
    "print('query:\\n', _seq_q)\n",
    "print('key:\\n', _seq_k)\n",
    "print('mask:\\n', _mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "もう一つはDecoder側でSelf Attentionを行う際に、各時刻で未来の情報に対するAttentionを行わないようにするマスクです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_attn_subsequent_mask(seq):\n",
    "    \"\"\"\n",
    "    未来の情報に対するattentionを0にするためのマスクを作成する\n",
    "    :param seq: tensor, size=(batch_size, length)\n",
    "    :return subsequent_mask: tensor, size=(batch_size, length, length)\n",
    "    \"\"\"\n",
    "    attn_shape = (seq.size(1), seq.size(1))\n",
    "    # 上三角行列(diagonal=1: 対角線より上が1で下が0)\n",
    "    subsequent_mask = torch.triu(torch.ones(attn_shape, dtype=torch.uint8, device=device), diagonal=1)\n",
    "    subsequent_mask = subsequent_mask.repeat(seq.size(0), 1, 1)\n",
    "    return subsequent_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iLectで実行する場合warning (GPU is too old) が出ますが, 実行に問題はないので気にせず進めてください.\n",
    "_seq = torch.tensor([[1,2,3,4]])\n",
    "_mask = get_attn_subsequent_mask(_seq)  # 行がquery、列がkeyに対応し、queryより未来のkeyの値が1で他は0の行列ができいる\n",
    "print('seq:\\n', _seq)\n",
    "print('mask:\\n', _mask)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. モデルの定義\n",
    "\n",
    "### Encoder\n",
    "これまで定義してきたサブレイヤーを統合して、Encoderを定義します。\n",
    "\n",
    "EncoderではSelf AttentionとPosition-Wise Feed Forward Networkからなるブロックを複数層繰り返すので、ブロックのクラスEncoderLayerを定義した後にEncoderを定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"Encoderのブロックのクラス\"\"\"\n",
    "    def __init__(self, d_model, d_inner_hid, n_head, d_k, d_v, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param d_model: int, 隠れ層の次元数\n",
    "        :param d_inner_hid: int, Position Wise Feed Forward Networkの隠れ層2層目の次元数\n",
    "        :param n_head: int,　ヘッド数\n",
    "        :param d_k: int, keyベクトルの次元数\n",
    "        :param d_v: int, valueベクトルの次元数\n",
    "        :param dropout: float, ドロップアウト率\n",
    "        \"\"\"\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        # Encoder内のSelf-Attention\n",
    "        self.slf_attn = MultiHeadAttention(\n",
    "            n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        # Postionwise FFN\n",
    "        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner_hid, dropout=dropout)\n",
    "\n",
    "    def forward(self, enc_input, slf_attn_mask=None):\n",
    "        \"\"\"\n",
    "        :param enc_input: tensor, Encoderの入力, \n",
    "            size=(batch_size, max_length, d_model)\n",
    "        :param slf_attn_mask: tensor, Self Attentionの行列にかけるマスク, \n",
    "            size=(batch_size, len_q, len_k)\n",
    "        :return enc_output: tensor, Encoderの出力, \n",
    "            size=(batch_size, max_length, d_model)\n",
    "        :return enc_slf_attn: tensor, EncoderのSelf Attentionの行列, \n",
    "            size=(n_head*batch_size, len_q, len_k)\n",
    "        \"\"\"\n",
    "        # Self-Attentionのquery, key, valueにはすべてEncoderの入力（enc_input）が入る\n",
    "        enc_output, enc_slf_attn = self.slf_attn(\n",
    "            enc_input, enc_input, enc_input, attn_mask=slf_attn_mask)\n",
    "        enc_output = self.pos_ffn(enc_output)\n",
    "        return enc_output, enc_slf_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"EncoderLayerブロックからなるEncoderのクラス\"\"\"\n",
    "    def __init__(\n",
    "            self, n_src_vocab, max_length, n_layers=6, n_head=8, d_k=64, d_v=64,\n",
    "            d_word_vec=512, d_model=512, d_inner_hid=1024, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param n_src_vocab: int, 入力言語の語彙数\n",
    "        :param max_length: int, 最大系列長\n",
    "        :param n_layers: int, レイヤー数\n",
    "        :param n_head: int,　ヘッド数\n",
    "        :param d_k: int, keyベクトルの次元数\n",
    "        :param d_v: int, valueベクトルの次元数\n",
    "        :param d_word_vec: int, 単語の埋め込みの次元数\n",
    "        :param d_model: int, 隠れ層の次元数\n",
    "        :param d_inner_hid: int, Position Wise Feed Forward Networkの隠れ層2層目の次元数\n",
    "        :param dropout: float, ドロップアウト率        \n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        n_position = max_length + 1\n",
    "        self.max_length = max_length\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Positional Encodingを用いたEmbedding\n",
    "        self.position_enc = nn.Embedding(n_position, d_word_vec, padding_idx=PAD)\n",
    "        self.position_enc.weight.data = position_encoding_init(n_position, d_word_vec)\n",
    "\n",
    "        # 一般的なEmbedding\n",
    "        self.src_word_emb = nn.Embedding(n_src_vocab, d_word_vec, padding_idx=PAD)\n",
    "\n",
    "        # EncoderLayerをn_layers個積み重ねる\n",
    "        self.layer_stack = nn.ModuleList([\n",
    "            EncoderLayer(d_model, d_inner_hid, n_head, d_k, d_v, dropout=dropout)\n",
    "            for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, src_seq, src_pos):\n",
    "        \"\"\"\n",
    "        :param src_seq: tensor, 入力系列, \n",
    "            size=(batch_size, max_length)\n",
    "        :param src_pos: tensor, 入力系列の各単語の位置情報,\n",
    "            size=(batch_size, max_length)\n",
    "        :return enc_output: tensor, Encoderの最終出力, \n",
    "            size=(batch_size, max_length, d_model)\n",
    "        :return enc_slf_attns: list, EncoderのSelf Attentionの行列のリスト\n",
    "        \"\"\"\n",
    "        # 一般的な単語のEmbeddingを行う\n",
    "        enc_input = self.src_word_emb(src_seq)\n",
    "        # Positional EncodingのEmbeddingを加算する\n",
    "        enc_input += self.position_enc(src_pos)\n",
    "\n",
    "        enc_slf_attns = []\n",
    "        enc_output = enc_input\n",
    "        # key(=enc_input)のPADに対応する部分のみ1のマスクを作成\n",
    "        enc_slf_attn_mask = get_attn_padding_mask(src_seq, src_seq)\n",
    "\n",
    "        # n_layers個のEncoderLayerに入力を通す\n",
    "        for enc_layer in self.layer_stack:\n",
    "            enc_output, enc_slf_attn = enc_layer(\n",
    "                enc_output, slf_attn_mask=enc_slf_attn_mask)\n",
    "            enc_slf_attns += [enc_slf_attn]\n",
    "\n",
    "        return enc_output, enc_slf_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "Deocoderも同様にSelf Attention, Source-Target Attention, Position-Wise Feed Forward Networkからなるブロックを複数層繰り返ので、ブロックのクラスDecoderLayerを定義した後にDecoderを定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"Decoderのブロックのクラス\"\"\"\n",
    "    def __init__(self, d_model, d_inner_hid, n_head, d_k, d_v, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param d_model: int, 隠れ層の次元数\n",
    "        :param d_inner_hid: int, Position Wise Feed Forward Networkの隠れ層2層目の次元数\n",
    "        :param n_head: int,　ヘッド数\n",
    "        :param d_k: int, keyベクトルの次元数\n",
    "        :param d_v: int, valueベクトルの次元数\n",
    "        :param dropout: float, ドロップアウト率\n",
    "        \"\"\"\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        # Decoder内のSelf-Attention\n",
    "        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        # Encoder-Decoder間のSource-Target Attention\n",
    "        self.enc_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        # Positionwise FFN\n",
    "        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner_hid, dropout=dropout)\n",
    "\n",
    "    def forward(self, dec_input, enc_output, slf_attn_mask=None, dec_enc_attn_mask=None):\n",
    "        \"\"\"\n",
    "        :param dec_input: tensor, Decoderの入力, \n",
    "            size=(batch_size, max_length, d_model)\n",
    "        :param enc_output: tensor, Encoderの出力, \n",
    "            size=(batch_size, max_length, d_model)\n",
    "        :param slf_attn_mask: tensor, Self Attentionの行列にかけるマスク, \n",
    "            size=(batch_size, len_q, len_k)\n",
    "        :param dec_enc_attn_mask: tensor, Soutce-Target Attentionの行列にかけるマスク, \n",
    "            size=(batch_size, len_q, len_k)\n",
    "        :return dec_output: tensor, Decoderの出力, \n",
    "            size=(batch_size, max_length, d_model)\n",
    "        :return dec_slf_attn: tensor, DecoderのSelf Attentionの行列, \n",
    "            size=(n_head*batch_size, len_q, len_k)\n",
    "        :return dec_enc_attn: tensor, DecoderのSoutce-Target Attentionの行列, \n",
    "            size=(n_head*batch_size, len_q, len_k)\n",
    "        \"\"\"\n",
    "        # Self-Attentionのquery, key, valueにはすべてDecoderの入力（dec_input）が入る\n",
    "        dec_output, dec_slf_attn = # WRITE ME!\n",
    "        \n",
    "        # Source-Target AttentionのqueryにはDecoderの出力(dec_output), key, valueにはEncoderの出力（enc_output）が入る\n",
    "        dec_output, dec_enc_attn = # WRITE ME!\n",
    "        \n",
    "        # Positionwise FFNに通す\n",
    "        dec_output = # WRITE ME!\n",
    "\n",
    "        return dec_output, dec_slf_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"DecoderLayerブロックからなるDecoderのクラス\"\"\"\n",
    "    def __init__(\n",
    "            self, n_tgt_vocab, max_length, n_layers=6, n_head=8, d_k=64, d_v=64,\n",
    "            d_word_vec=512, d_model=512, d_inner_hid=1024, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param n_tgt_vocab: int, 出力言語の語彙数\n",
    "        :param max_length: int, 最大系列長\n",
    "        :param n_layers: int, レイヤー数\n",
    "        :param n_head: int,　ヘッド数\n",
    "        :param d_k: int, keyベクトルの次元数\n",
    "        :param d_v: int, valueベクトルの次元数\n",
    "        :param d_word_vec: int, 単語の埋め込みの次元数\n",
    "        :param d_model: int, 隠れ層の次元数\n",
    "        :param d_inner_hid: int, Position Wise Feed Forward Networkの隠れ層2層目の次元数\n",
    "        :param dropout: float, ドロップアウト率        \n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        n_position = max_length + 1\n",
    "        self.max_length = max_length\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Positional Encodingを用いたEmbedding\n",
    "        self.position_enc = nn.Embedding(\n",
    "            n_position, d_word_vec, padding_idx=PAD)\n",
    "        self.position_enc.weight.data = position_encoding_init(n_position, d_word_vec)\n",
    "\n",
    "        # 一般的なEmbedding\n",
    "        self.tgt_word_emb = nn.Embedding(\n",
    "            n_tgt_vocab, d_word_vec, padding_idx=PAD)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # DecoderLayerをn_layers個積み重ねる\n",
    "        self.layer_stack = nn.ModuleList([\n",
    "            DecoderLayer(d_model, d_inner_hid, n_head, d_k, d_v, dropout=dropout)\n",
    "            for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, tgt_seq, tgt_pos, src_seq, enc_output):\n",
    "        \"\"\"\n",
    "        :param tgt_seq: tensor, 出力系列, \n",
    "            size=(batch_size, max_length)\n",
    "        :param tgt_pos: tensor, 出力系列の各単語の位置情報,\n",
    "            size=(batch_size, max_length)\n",
    "        :param src_seq: tensor, 入力系列, \n",
    "            size=(batch_size, n_src_vocab)\n",
    "        :param enc_output: tensor, Encoderの出力, \n",
    "            size=(batch_size, max_length, d_model)\n",
    "        :return dec_output: tensor, Decoderの最終出力, \n",
    "            size=(batch_size, max_length, d_model)\n",
    "        :return dec_slf_attns: list, DecoderのSelf Attentionの行列のリスト \n",
    "        :return dec_slf_attns: list, DecoderのSelf Attentionの行列のリスト\n",
    "        \"\"\"\n",
    "        # 一般的な単語のEmbeddingを行う\n",
    "        dec_input = self.tgt_word_emb(tgt_seq)\n",
    "        # Positional EncodingのEmbeddingを加算する\n",
    "        dec_input += self.position_enc(tgt_pos)\n",
    "\n",
    "        # Self-Attention用のマスクを作成\n",
    "        # key(=dec_input)のPADに対応する部分が1のマスクと、queryから見たkeyの未来の情報に対応する部分が1のマスクのORをとる\n",
    "        dec_slf_attn_pad_mask = get_attn_padding_mask(tgt_seq, tgt_seq)  # (N, max_length, max_length)\n",
    "        dec_slf_attn_sub_mask = get_attn_subsequent_mask(tgt_seq)  # (N, max_length, max_length)\n",
    "        dec_slf_attn_mask = torch.gt(dec_slf_attn_pad_mask + dec_slf_attn_sub_mask, 0)  # ORをとる\n",
    "\n",
    "        # key(=dec_input)のPADに対応する部分のみ1のマスクを作成\n",
    "        dec_enc_attn_pad_mask = get_attn_padding_mask(tgt_seq, src_seq)  # (N, max_length, max_length)\n",
    "\n",
    "        dec_slf_attns, dec_enc_attns = [], []\n",
    "\n",
    "        dec_output = dec_input\n",
    "        # n_layers個のDecoderLayerに入力を通す\n",
    "        for dec_layer in self.layer_stack:\n",
    "            dec_output, dec_slf_attn, dec_enc_attn = dec_layer(\n",
    "                dec_output, enc_output,\n",
    "                slf_attn_mask=dec_slf_attn_mask,\n",
    "                dec_enc_attn_mask=dec_enc_attn_pad_mask)\n",
    "\n",
    "            dec_slf_attns += [dec_slf_attn]\n",
    "            dec_enc_attns += [dec_enc_attn]\n",
    "\n",
    "        return dec_output, dec_slf_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer\n",
    "最後にモデル全体の機構を定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"Transformerのモデル全体のクラス\"\"\"\n",
    "    def __init__(\n",
    "            self, n_src_vocab, n_tgt_vocab, max_length, n_layers=6, n_head=8,\n",
    "            d_word_vec=512, d_model=512, d_inner_hid=1024, d_k=64, d_v=64,\n",
    "            dropout=0.1, proj_share_weight=True):\n",
    "        \"\"\"\n",
    "        :param n_src_vocab: int, 入力言語の語彙数\n",
    "        :param n_tgt_vocab: int, 出力言語の語彙数\n",
    "        :param max_length: int, 最大系列長\n",
    "        :param n_layers: int, レイヤー数\n",
    "        :param n_head: int,　ヘッド数\n",
    "        :param d_k: int, keyベクトルの次元数\n",
    "        :param d_v: int, valueベクトルの次元数\n",
    "        :param d_word_vec: int, 単語の埋め込みの次元数\n",
    "        :param d_model: int, 隠れ層の次元数\n",
    "        :param d_inner_hid: int, Position Wise Feed Forward Networkの隠れ層2層目の次元数\n",
    "        :param dropout: float, ドロップアウト率        \n",
    "        :param proj_share_weight: bool, 出力言語の単語のEmbeddingと出力の写像で重みを共有する        \n",
    "        \"\"\"\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(\n",
    "            n_src_vocab, max_length, n_layers=n_layers, n_head=n_head,\n",
    "            d_word_vec=d_word_vec, d_model=d_model,\n",
    "            d_inner_hid=d_inner_hid, dropout=dropout)\n",
    "        self.decoder = Decoder(\n",
    "            n_tgt_vocab, max_length, n_layers=n_layers, n_head=n_head,\n",
    "            d_word_vec=d_word_vec, d_model=d_model,\n",
    "            d_inner_hid=d_inner_hid, dropout=dropout)\n",
    "        self.tgt_word_proj = nn.Linear(d_model, n_tgt_vocab, bias=False)\n",
    "        nn.init.xavier_normal_(self.tgt_word_proj.weight)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        assert d_model == d_word_vec  # 各モジュールの出力のサイズは揃える\n",
    "\n",
    "        if proj_share_weight:\n",
    "            # 出力言語の単語のEmbeddingと出力の写像で重みを共有する\n",
    "            assert d_model == d_word_vec\n",
    "            self.tgt_word_proj.weight = self.decoder.tgt_word_emb.weight\n",
    "\n",
    "    def get_trainable_parameters(self):\n",
    "        # Positional Encoding以外のパラメータを更新する\n",
    "        enc_freezed_param_ids = set(map(id, self.encoder.position_enc.parameters()))\n",
    "        dec_freezed_param_ids = set(map(id, self.decoder.position_enc.parameters()))\n",
    "        freezed_param_ids = enc_freezed_param_ids | dec_freezed_param_ids\n",
    "        return (p for p in self.parameters() if id(p) not in freezed_param_ids)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_seq, src_pos = src\n",
    "        tgt_seq, tgt_pos = tgt\n",
    "\n",
    "        src_seq = src_seq[:, 1:]\n",
    "        src_pos = src_pos[:, 1:]\n",
    "        tgt_seq = tgt_seq[:, :-1]\n",
    "        tgt_pos = tgt_pos[:, :-1]\n",
    "\n",
    "        enc_output, *_ = self.encoder(src_seq, src_pos)\n",
    "        dec_output, *_ = self.decoder(tgt_seq, tgt_pos, src_seq, enc_output)\n",
    "        seq_logit = self.tgt_word_proj(dec_output)\n",
    "\n",
    "        return seq_logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss(batch_X, batch_Y, model, criterion, optimizer=None, is_train=True):\n",
    "    # バッチの損失を計算\n",
    "    model.train(is_train)\n",
    "    \n",
    "    pred_Y = model(batch_X, batch_Y)\n",
    "    gold = batch_Y[0][:, 1:].contiguous()\n",
    "    loss = criterion(pred_Y.view(-1, pred_Y.size(2)), gold.view(-1))\n",
    "\n",
    "    if is_train:  # 訓練時はパラメータを更新\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    gold = gold.data.cpu().numpy().tolist()\n",
    "    pred = pred_Y.max(dim=-1)[1].data.cpu().numpy().tolist()\n",
    "\n",
    "    return loss.item(), gold, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 20\n",
    "batch_size = 64\n",
    "num_epochs = 15\n",
    "lr = 0.001\n",
    "ckpt_path = 'transformer.pth'\n",
    "max_length = MAX_LENGTH + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_args = {\n",
    "    'n_src_vocab': vocab_size_X,\n",
    "    'n_tgt_vocab': vocab_size_Y,\n",
    "    'max_length': max_length,\n",
    "    'proj_share_weight': True,\n",
    "    'd_k': 32,\n",
    "    'd_v': 32,\n",
    "    'd_model': 128,\n",
    "    'd_word_vec': 128,\n",
    "    'd_inner_hid': 256,\n",
    "    'n_layers': 3,\n",
    "    'n_head': 6,\n",
    "    'dropout': 0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DataLoaderやモデルを定義\n",
    "train_dataloader = DataLoader(\n",
    "    train_X, train_Y, batch_size\n",
    "    )\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_X, valid_Y, batch_size, \n",
    "    shuffle=False\n",
    "    )\n",
    "\n",
    "model = Transformer(**model_args).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.get_trainable_parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD, reduction='sum').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_bleu(refs, hyps):\n",
    "    \"\"\"\n",
    "    BLEUスコアを計算する関数\n",
    "    :param refs: list, 参照訳。単語のリストのリスト (例： [['I', 'have', 'a', 'pen'], ...])\n",
    "    :param hyps: list, モデルの生成した訳。単語のリストのリスト (例： [['I', 'have', 'a', 'pen'], ...])\n",
    "    :return: float, BLEUスコア(0~100)\n",
    "    \"\"\"\n",
    "    refs = [[ref[:ref.index(EOS)]] for ref in refs]\n",
    "    hyps = [hyp[:hyp.index(EOS)] if EOS in hyp else hyp for hyp in hyps]\n",
    "    return 100 * bleu_score.corpus_bleu(refs, hyps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練\n",
    "best_valid_bleu = 0.\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    start = time.time()\n",
    "    train_loss = 0.\n",
    "    train_refs = []\n",
    "    train_hyps = []\n",
    "    valid_loss = 0.\n",
    "    valid_refs = []\n",
    "    valid_hyps = []\n",
    "    # train\n",
    "    for batch in train_dataloader:\n",
    "        batch_X, batch_Y = batch\n",
    "        loss, gold, pred = compute_loss(\n",
    "            batch_X, batch_Y, model, criterion, optimizer, is_train=True\n",
    "            )\n",
    "        train_loss += loss\n",
    "        train_refs += gold\n",
    "        train_hyps += pred\n",
    "    # valid\n",
    "    for batch in valid_dataloader:\n",
    "        batch_X, batch_Y = batch\n",
    "        loss, gold, pred = compute_loss(\n",
    "            batch_X, batch_Y, model, criterion, is_train=False\n",
    "            )\n",
    "        valid_loss += loss\n",
    "        valid_refs += gold\n",
    "        valid_hyps += pred\n",
    "    # 損失をサンプル数で割って正規化\n",
    "    train_loss /= len(train_dataloader.data) \n",
    "    valid_loss /= len(valid_dataloader.data) \n",
    "    # BLEUを計算\n",
    "    train_bleu = calc_bleu(train_refs, train_hyps)\n",
    "    valid_bleu = calc_bleu(valid_refs, valid_hyps)\n",
    "\n",
    "    # validationデータでBLEUが改善した場合にはモデルを保存\n",
    "    if valid_bleu > best_valid_bleu:\n",
    "        ckpt = model.state_dict()\n",
    "        torch.save(ckpt, ckpt_path)\n",
    "        best_valid_bleu = valid_bleu\n",
    "\n",
    "    elapsed_time = (time.time()-start) / 60\n",
    "    print('Epoch {} [{:.1f}min]: train_loss: {:5.2f}  train_bleu: {:2.2f}  valid_loss: {:5.2f}  valid_bleu: {:2.2f}'.format(\n",
    "            epoch, elapsed_time, train_loss, train_bleu, valid_loss, valid_bleu))\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(model, src, max_length=20):\n",
    "    # 学習済みモデルで系列を生成する\n",
    "    model.eval()\n",
    "    \n",
    "    src_seq, src_pos = src\n",
    "    batch_size = src_seq.size(0)\n",
    "    enc_output, enc_slf_attns = model.encoder(src_seq, src_pos)\n",
    "        \n",
    "    tgt_seq = torch.full([batch_size, 1], BOS, dtype=torch.long, device=device)\n",
    "    tgt_pos = torch.arange(1, dtype=torch.long, device=device)\n",
    "    tgt_pos = tgt_pos.unsqueeze(0).repeat(batch_size, 1)\n",
    "\n",
    "    # 時刻ごとに処理\n",
    "    for t in range(1, max_length+1):\n",
    "        dec_output, dec_slf_attns, dec_enc_attns = model.decoder(\n",
    "            tgt_seq, tgt_pos, src_seq, enc_output)\n",
    "        dec_output = model.tgt_word_proj(dec_output)\n",
    "        out = dec_output[:, -1, :].max(dim=-1)[1].unsqueeze(1)\n",
    "        # 自身の出力を次の時刻の入力にする\n",
    "        tgt_seq = torch.cat([tgt_seq, out], dim=-1)\n",
    "        tgt_pos = torch.arange(t+1, dtype=torch.long, device=device)\n",
    "        tgt_pos = tgt_pos.unsqueeze(0).repeat(batch_size, 1)\n",
    "\n",
    "    return tgt_seq[:, 1:], enc_slf_attns, dec_slf_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ids_to_sentence(vocab, ids):\n",
    "    # IDのリストを単語のリストに変換する\n",
    "    return [vocab.id2word[_id] for _id in ids]\n",
    "\n",
    "def trim_eos(ids):\n",
    "    # IDのリストからEOS以降の単語を除外する\n",
    "    if EOS in ids:\n",
    "        return ids[:ids.index(EOS)]\n",
    "    else:\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 学習済みモデルの読み込み\n",
    "model = Transformer(**model_args).to(device)\n",
    "ckpt = torch.load(ckpt_path)\n",
    "model.load_state_dict(ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# テストデータの読み込み\n",
    "test_X = load_data('../data/dev.en')\n",
    "test_Y = load_data('../data/dev.ja')\n",
    "test_X = [sentence_to_ids(vocab_X, sentence) for sentence in test_X]\n",
    "test_Y = [sentence_to_ids(vocab_Y, sentence) for sentence in test_Y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(\n",
    "    test_X, test_Y, 1,\n",
    "    shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "src, tgt = next(test_dataloader)\n",
    "\n",
    "src_ids = src[0][0].cpu().numpy()\n",
    "tgt_ids = tgt[0][0].cpu().numpy()\n",
    "\n",
    "print('src: {}'.format(' '.join(ids_to_sentence(vocab_X, src_ids[1:-1]))))\n",
    "print('tgt: {}'.format(' '.join(ids_to_sentence(vocab_Y, tgt_ids[1:-1]))))\n",
    "\n",
    "preds, enc_slf_attns, dec_slf_attns, dec_enc_attns = test(model, src)\n",
    "pred_ids = preds[0].data.cpu().numpy().tolist()\n",
    "print('out: {}'.format(' '.join(ids_to_sentence(vocab_Y, trim_eos(pred_ids)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEUの評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# BLEUの評価\n",
    "test_dataloader = DataLoader(\n",
    "    test_X, test_Y, 128,\n",
    "    shuffle=False\n",
    "    )\n",
    "refs_list = []\n",
    "hyp_list = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    batch_X, batch_Y = batch\n",
    "    preds, *_ = test(model, batch_X)\n",
    "    preds = preds.data.cpu().numpy().tolist()\n",
    "    refs = batch_Y[0].data.cpu().numpy()[:, 1:].tolist()\n",
    "    refs_list += refs\n",
    "    hyp_list += preds\n",
    "bleu = calc_bleu(refs_list, hyp_list)\n",
    "print(bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Attentionの可視化\n",
    "\n",
    "学習済みモデルのAttentionを可視化します。\n",
    "\n",
    "以下は元論文の図で、Encoder側のあるレイヤーにおけるSelf-Attentionを可視化したものです。\n",
    "\n",
    "異なる色が異なるヘッドを、色の濃淡がAttentionの重みを表しており、各ヘッドが異なる位置へのAttentionを担っていることがわかります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/AttentionVisualization.png\" style=\"height:400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下ではもう少し単純に、各行をquery、各列をkeyとしてAttentionの行列を可視化します。\n",
    "\n",
    "モデルはEncoder/Decoderそれぞれが複数のレイヤーを有し、各レイヤー内の各Attentionも複数のヘッドを有するので、順番に中身を可視化していきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_attentions(attention, sent_q, sent_k, n_max=10):\n",
    "    # Attentionの行列を可視化する(n_maxは表示するヘッドの最大数)\n",
    "    # EOS以降を削除\n",
    "    q_eos = sent_q.index(EOS_TOKEN)\n",
    "    k_eos = sent_k.index(EOS_TOKEN)\n",
    "    sent_q = sent_q[:q_eos+1]\n",
    "    sent_k = sent_k[:k_eos+1]\n",
    "    \n",
    "    n_head = attention.size(0)\n",
    "    n_head = min(n_head, n_max)\n",
    "    fig, axes = plt.subplots(1, n_head, figsize=(20, 8))\n",
    "    for h in range(n_head):\n",
    "        data = attention[h].cpu().data[:q_eos+1, :k_eos+1]\n",
    "        x = sent_k\n",
    "        y = sent_q if h == 0 else []\n",
    "        sns.heatmap(data, vmin=0., vmax=1., cbar=False, ax=axes[h], cmap='Blues')\n",
    "        axes[h].set_xticklabels(x)\n",
    "        axes[h].set_yticklabels(y, rotation=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_context('poster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_sent = [vocab_X.id2word[i] for i in src_ids[1:]]\n",
    "pred_sent = [vocab_Y.id2word[i] for i in pred_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 1st layer\n",
    "show_attentions(enc_slf_attns[0], src_sent, src_sent, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 2nd layer\n",
    "show_attentions(enc_slf_attns[1], src_sent, src_sent, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 3rd layer\n",
    "show_attentions(enc_slf_attns[2], src_sent, src_sent, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 1st layer\n",
    "show_attentions(dec_slf_attns[0], pred_sent, pred_sent, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 2nd layer\n",
    "show_attentions(dec_slf_attns[1], pred_sent, pred_sent, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 3rd layer\n",
    "show_attentions(dec_slf_attns[2], pred_sent, pred_sent, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Decoder Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 1st layer\n",
    "show_attentions(dec_enc_attns[0], pred_sent, src_sent, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 2nd layer\n",
    "show_attentions(dec_enc_attns[1], pred_sent, src_sent, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 3rd layer\n",
    "show_attentions(dec_enc_attns[2], pred_sent, src_sent, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
